{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Research_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gE8Mta4TJ7F0",
        "-_gs4fG4USj2",
        "fQi9boLiEaOB",
        "pyByjVPZE-r4",
        "YLIx7hncK55D",
        "wx1p5gtbznbR",
        "C7kB57xquZ0D",
        "Ca9ivVo7EwWk",
        "neXC_J2Hin10",
        "X1VH15Vz3ng1",
        "_HFlh136vBdd",
        "tfdXzDftY6pt",
        "rU4PGYZfmFUN",
        "aM-htCV_NR92",
        "8OVTJhppObIs",
        "tgRtoDdEQKT-",
        "5Gaw_J86QhzR",
        "3_ll_kk5oVGP",
        "6aRuZB1aqrA7"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZJJB646KuDBQ/8LoVP0W3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lilakhd/Spectral-and-K-means-clustering-of-patients-with-MDD/blob/main/Research_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL2m1zcDUO8P"
      },
      "source": [
        "# I) Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrUCTIXCPhV6"
      },
      "source": [
        "# Importing Dependencies for Data Import\n",
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SFxBhy0pQ6rH"
      },
      "source": [
        "# Importing data as pandas df\n",
        "uploaded = files.upload()\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['gendep.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8Mta4TJ7F0"
      },
      "source": [
        "# II) Exploring Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypdskjIMP_iB"
      },
      "source": [
        "# Preview data\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNmsE9BUvnF"
      },
      "source": [
        "# Dataset Shape\n",
        "i,j = df2.shape\n",
        "print(f\"A total of {i} participants were included in the initial dataset\")\n",
        "print(f\"A total of {j} columns were included in the initial dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcE5sIUUXEDw"
      },
      "source": [
        "# Descriptives for Whole Sample\n",
        "df2.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfnsxakDKziF"
      },
      "source": [
        "# Exploring which variables have missing values\n",
        "mi_col=[col for col in df2.columns if df2[col].isnull().any()]\n",
        "n_mi=len(mi_col)\n",
        "print(f\"{n_mi} variable has missing data\")\n",
        "print(f\"{mi_col} is the only variable with missing data\")\n",
        "# Number of subjects with missing PRS\n",
        "prs_mi = df2[\"prs\"].isna().sum()\n",
        "print(f\"A total of {prs_mi} subjects have missing PRS values.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqzOopC_JazT"
      },
      "source": [
        "# Omitting subjects with missing PRS \n",
        "df3 = df2.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyhO6sbCJrv_"
      },
      "source": [
        "# Descriptives for Whole Sample (minus those with missing PRS)\n",
        "df3.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiTSQ0t-FeRw"
      },
      "source": [
        "# Descriptives by Drug Assignment (minus those with missing PRS)\n",
        "df3.groupby(\"drug\").describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_gs4fG4USj2"
      },
      "source": [
        "# III) Subsetting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiH7MYAEUVyZ"
      },
      "source": [
        "# 1. Subsetting Whole Sample Data\n",
        "df_all = df3.drop(['centreid','subjectid', 'Row.names','bloodsampleid.x'], axis=1)\n",
        "# 2. Subsetting Escitalopram Sample (A)\n",
        "df_esc = df3.drop(['centreid', 'subjectid','Row.names','bloodsampleid.x'], axis=1)\n",
        "df_esc = df_esc.loc[df_esc['drug'] == 2]\n",
        "df_esc = df_esc.drop(['drug'],axis=1)\n",
        "# 3. Subsetting Nortriptyline Sample (B)\n",
        "df_nor = df3.drop(['centreid', 'subjectid', 'Row.names','bloodsampleid.x'], axis=1)\n",
        "df_nor = df_nor.loc[df_nor['drug'] == 1]\n",
        "df_nor = df_nor.drop(['drug'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyWJZnw-ZVNA"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQi9boLiEaOB"
      },
      "source": [
        "# IV) Defining Feature Spaces/Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyByjVPZE-r4"
      },
      "source": [
        "## A. All Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkD6_tiEdBk"
      },
      "source": [
        "# Raw Data: Features Only\n",
        "X_all = df_all.drop([\"mdpercadj\", \"hdremit.all\"], axis=1)\n",
        "X_all.head()\n",
        "\n",
        "# Standardized & Scaled Features Only\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler\n",
        "X_scaled_all = sc().fit_transform(X_all)\n",
        "\n",
        "  \n",
        "# Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA \n",
        "## PCA n_components = 2 \n",
        "pca = PCA(n_components = 2) \n",
        "X_principal2_all = pca.fit_transform(X_scaled_all) \n",
        "X_principal2_all = pd.DataFrame(X_principal2_all) \n",
        "X_principal2_all.columns = ['P1', 'P2'] \n",
        "X_principal2_all.head(2) \n",
        "\n",
        "## PCA n_components optimised\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca = PCA().fit(X_scaled_all)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title('All Sample (N=421)')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        " ### n_components = 90 ###\n",
        "\n",
        "pca90 = PCA(n_components = 90) \n",
        "X_principal90_all = pca90.fit_transform(X_scaled_all) \n",
        "X_principal90_all = pd.DataFrame(X_principal90_all) \n",
        "X_principal90_all.columns = ['P%d' % i for i in range(1, 91, 1)]\n",
        "X_principal90_all.head(2) \n",
        "\n",
        "# Saving PCA plot\n",
        "#from google.colab import files\n",
        "#plt.savefig(\"all_PCA_n_selection.png\")\n",
        "#files.download(\"all_PCA_n_selection.png\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLOnsuqzTTxB"
      },
      "source": [
        "# Labels (Outcomes):\n",
        "Y_all = df_all[[\"mdpercadj\", \"hdremit.all\"]]\n",
        "Y_all.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLIx7hncK55D"
      },
      "source": [
        "## B. Esciltalopram Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zqUKVUuK8Tp"
      },
      "source": [
        "# Raw Data: Features Only\n",
        "X_esc = df_esc.drop([\"mdpercadj\", \"hdremit.all\"], axis=1)\n",
        "X_esc.head()\n",
        "\n",
        "# Standardized & Scaled Features Only\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler\n",
        "X_scaled_esc = sc().fit_transform(X_esc)\n",
        "\n",
        "  \n",
        "# Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA \n",
        "## PCA n_components = 2 \n",
        "pca = PCA(n_components = 2) \n",
        "X_principal2_esc = pca.fit_transform(X_scaled_esc) \n",
        "X_principal2_esc = pd.DataFrame(X_principal2_esc) \n",
        "X_principal2_esc.columns = ['P1', 'P2'] \n",
        "X_principal2_esc.head(2) \n",
        "\n",
        "## PCA n_components optimised\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca_esc = PCA().fit(X_scaled_esc)\n",
        "plt.plot(np.cumsum(pca_esc.explained_variance_ratio_))\n",
        "plt.title('Escitalopram Sample (n=217)')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "fig1 = plt.gcf()\n",
        "plt.show()\n",
        "fig1.savefig('esc_PCA_n_selection')\n",
        "\n",
        "\n",
        " ### n_components = 90 ###\n",
        "\n",
        "pca90_esc = PCA(n_components = 90) \n",
        "X_principal90_esc = pca90_esc.fit_transform(X_scaled_esc) \n",
        "X_principal90_esc = pd.DataFrame(X_principal90_esc) \n",
        "X_principal90_esc.columns = ['P%d' % i for i in range(1, 91, 1)]\n",
        "X_principal90_esc.head(2) \n",
        "\n",
        "# Sving PCA plot\n",
        "#from google.colab import files\n",
        "#files.download(\"esc_PCA_n_selection.png\") \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofQ_PqV6TlBJ"
      },
      "source": [
        "# Labels (Outcomes):\n",
        "Y_esc = df_esc[[\"mdpercadj\", \"hdremit.all\"]]\n",
        "Y_esc.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZW1vFx5MgeR"
      },
      "source": [
        "## C. Nortryptaline Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZCKyKdE6JVF"
      },
      "source": [
        "# Raw Data: Features Only\n",
        "X_nor = df_nor.drop([\"mdpercadj\", \"hdremit.all\"], axis=1)\n",
        "X_nor.head()\n",
        "\n",
        "# Standardized & Scaled Features Only\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler\n",
        "X_scaled_nor = sc().fit_transform(X_nor)\n",
        "\n",
        "  \n",
        "# Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA \n",
        "## PCA n_components = 2 \n",
        "pca_nor = PCA(n_components = 2) \n",
        "X_principal2_nor = pca_nor.fit_transform(X_scaled_nor) \n",
        "X_principal2_nor = pd.DataFrame(X_principal2_nor) \n",
        "X_principal2_nor.columns = ['P1', 'P2'] \n",
        "X_principal2_nor.head(2) \n",
        "\n",
        "## PCA n_components optimised\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pca_nor = PCA().fit(X_scaled_nor)\n",
        "plt.plot(np.cumsum(pca_nor.explained_variance_ratio_))\n",
        "plt.title('Nortryptaline Sample (n=204)')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "fig2= plt.gcf()\n",
        "plt.show()\n",
        "fig2.savefig('nor_PCA_n_selection')\n",
        "\n",
        "\n",
        " ### n_components = 90 ###\n",
        "\n",
        "pca90_nor = PCA(n_components = 90) \n",
        "X_principal90_nor = pca90_esc.fit_transform(X_scaled_nor) \n",
        "X_principal90_nor = pd.DataFrame(X_principal90_nor) \n",
        "X_principal90_nor.columns = ['P%d' % i for i in range(1, 91, 1)]\n",
        "X_principal90_nor.head(2) \n",
        "\n",
        "# Saving plot\n",
        "# from google.colab import files\n",
        "# files.download(\"nor_PCA_n_selection.png\") \n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx1p5gtbznbR"
      },
      "source": [
        "# V) Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Haa8phCnDSMj"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn import metrics\n",
        "random_state = 123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7kB57xquZ0D"
      },
      "source": [
        "## A. K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca9ivVo7EwWk"
      },
      "source": [
        "### i. All Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoKhPVXii_Q3"
      },
      "source": [
        "## K-Means iterated over cluster size = 2 through cluster size ##\n",
        "## = 7 for each of the feature sets for the whole sample.      ##\n",
        "\n",
        "# Raw Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_all_raw = []\n",
        "HM_Km_all_raw = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=123)\n",
        "    km = km.fit(X_all)\n",
        "    #lbls = km.predict(X_all)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_all,  km.predict(X_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_all_raw.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_all), df_all['hdremit.all']),3)\n",
        "    HM_Km_all_raw.append(hm_score)\n",
        "\n",
        "# Scaled Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_all_scaled = []\n",
        "HM_Km_all_scaled = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=123)\n",
        "    km = km.fit(X_scaled_all)\n",
        "    #lbls = km.predict(X_scaled_all)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_scaled_all,  km.predict(X_scaled_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_all_scaled.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_scaled_all), df_all['hdremit.all']),3)\n",
        "    HM_Km_all_scaled.append(hm_score)\n",
        "        \n",
        "\n",
        "# PCA (n = 2)\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_all_pca2 = []\n",
        "HM_Km_all_pca2 = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=123)\n",
        "    km = km.fit(X_principal2_all)\n",
        "    lbls = km.predict(X_principal2_all)  # storing labels for this run because it was selected as optimal feature set\n",
        "    df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal2_all,  km.predict(X_principal2_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_all_pca2.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_principal2_all), df_all['hdremit.all']),3)\n",
        "    HM_Km_all_pca2.append(hm_score)   \n",
        "\n",
        "# PCA (n=90)     \n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_all_pca90 = []\n",
        "HM_Km_all_pca90 = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=123)\n",
        "    km = km.fit(X_principal90_all)\n",
        "    #lbls = km.predict(X_principal90_all)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal90_all,  km.predict(X_principal90_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_all_pca90.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_principal90_all), df_all['hdremit.all']),3)\n",
        "    HM_Km_all_pca90.append(hm_score)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMLOSVkM7IyB"
      },
      "source": [
        "# Summary of homogeneity and silhoutte across feature sets in escitalopram sample\n",
        "pd.DataFrame({'ClusterSize' : [2,3,4,5,6,7],\n",
        "              'AvgSH_raw': SH_Km_all_raw,\n",
        "              'Homogen_raw': HM_Km_all_raw,\n",
        "              'AvgSH_scaled': SH_Km_all_scaled,\n",
        "              'Homogen_scaled': HM_Km_all_scaled,\n",
        "              'AvgSH_pca2': SH_Km_all_pca2,\n",
        "              'Homogen_pca2': HM_Km_all_pca2,\n",
        "              'AvgSH_pca90': SH_Km_all_pca90,\n",
        "              'Homogen_pca90': HM_Km_all_pca90\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysr_qlmgE7-v"
      },
      "source": [
        "PCA n=2 generally showed best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neXC_J2Hin10"
      },
      "source": [
        "### ii. Escitalopram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IKNmKKinhv"
      },
      "source": [
        "## K-Means iterated over cluster size = 2 through cluster size    ##\n",
        "## = 7 for each of the feature sets for the escitalopram sample.  ##\n",
        "\n",
        "# Raw Data \n",
        "K = range(2,8,1)\n",
        "SH_Km_esc_raw = []\n",
        "HM_Km_esc_raw = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=123)\n",
        "    km = km.fit(X_esc)\n",
        "    #lbls = km.predict(X_all)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_esc,  km.predict(X_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_esc_raw.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_esc), df_esc['hdremit.all']),3)\n",
        "    HM_Km_esc_raw.append(hm_score)\n",
        "\n",
        "# Scaled Data\n",
        "K = range(2,8,1)\n",
        "SH_Km_esc_scaled = []\n",
        "HM_Km_esc_scaled = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_scaled_esc)\n",
        "    #lbls = km.predict(X_scaled_esc)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_scaled_esc,  km.predict(X_scaled_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_esc_scaled.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_scaled_esc), df_esc['hdremit.all']),3)\n",
        "    HM_Km_esc_scaled.append(hm_score)\n",
        "\n",
        "# PCA n = 2\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_esc_pca2 = []\n",
        "HM_Km_esc_pca2 = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_principal2_esc)\n",
        "    lbls = km.predict(X_principal2_esc) # labels stored because best performance across feature sets\n",
        "    df_esc['km_cl_esc_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal2_esc,  km.predict(X_principal2_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_esc_pca2.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_principal2_esc), df_esc['hdremit.all']),3)\n",
        "    HM_Km_esc_pca2.append(hm_score)\n",
        "\n",
        "# PCA n = 90\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_esc_pca90 = []\n",
        "HM_Km_esc_pca90 = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_principal90_esc)\n",
        "    #lbls = km.predict(X_principal90_esc)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal90_esc,  km.predict(X_principal90_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_esc_pca90.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_principal90_esc), df_esc['hdremit.all']),3)\n",
        "    HM_Km_esc_pca90.append(hm_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5MMF3OsFxN6"
      },
      "source": [
        "# Summary of homogeneity and silhoutte across feature sets in escitalopram sample\n",
        "pd.DataFrame({'ClusterSize' : [2,3,4,5,6,7],\n",
        "              'AvgSH_raw': SH_Km_esc_raw,\n",
        "              'Homogen_raw': HM_Km_esc_raw,\n",
        "              'AvgSH_scaled': SH_Km_esc_scaled,\n",
        "              'Homogen_scaled': HM_Km_esc_scaled,\n",
        "              'AvgSH_pca2': SH_Km_esc_pca2,\n",
        "              'Homogen_pca2': HM_Km_esc_pca2,\n",
        "              'AvgSH_pca90': SH_Km_esc_pca90,\n",
        "              'Homogen_pca90': HM_Km_esc_pca90\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1VH15Vz3ng1"
      },
      "source": [
        "### iii. Nortryptaline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvOE8aICG8fI"
      },
      "source": [
        "## K-Means iterated over cluster size = 2 through cluster size    ##\n",
        "## = 7 for each of the feature sets for the noritalopram sample.  ##\n",
        "\n",
        "# Raw Data \n",
        "K = range(2,8,1)\n",
        "SH_Km_nor_raw = []\n",
        "HM_Km_nor_raw = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_nor)\n",
        "    #lbls = km.predict(X_all)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_nor,  km.predict(X_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_nor_raw.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_nor), df_nor['hdremit.all']),3)\n",
        "    HM_Km_nor_raw.append(hm_score)\n",
        "\n",
        "# Scaled Data\n",
        "K = range(2,8,1)\n",
        "SH_Km_nor_scaled = []\n",
        "HM_Km_nor_scaled = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_scaled_nor)\n",
        "    #lbls = km.predict(X_scaled_nor)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_scaled_nor,  km.predict(X_scaled_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_nor_scaled.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_scaled_nor), df_nor['hdremit.all']),3)\n",
        "    HM_Km_nor_scaled.append(hm_score)\n",
        "\n",
        "# PCA n = 2\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_nor_pca2 = []\n",
        "HM_Km_nor_pca2 = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_principal2_nor)\n",
        "    lbls = km.predict(X_principal2_nor) # labels stored because best performance across feature sets\n",
        "    df_nor['km_cl_nor_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal2_nor,  km.predict(X_principal2_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_nor_pca2.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_principal2_nor), df_nor['hdremit.all']),3)\n",
        "    HM_Km_nor_pca2.append(hm_score)\n",
        "\n",
        "# PCA n = 90\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_Km_nor_pca90 = []\n",
        "HM_Km_nor_pca90 = []\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k,random_state=random_state)\n",
        "    km = km.fit(X_principal90_nor)\n",
        "    #lbls = km.predict(X_principal90_nor)\n",
        "    #df_all['km_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal90_nor,  km.predict(X_principal90_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_Km_nor_pca90.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(km.predict(X_principal90_nor), df_nor['hdremit.all']),3)\n",
        "    HM_Km_nor_pca90.append(hm_score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hptcLYOjG_aN"
      },
      "source": [
        "# Summary of homogeneity and silhoutte across feature sets in nortryptaline sample\n",
        "pd.DataFrame({'ClusterSize' : [2,3,4,5,6,7],\n",
        "              'AvgSH_raw': SH_Km_nor_raw,\n",
        "              'Homogen_raw': HM_Km_nor_raw,\n",
        "              'AvgSH_scaled': SH_Km_nor_scaled,\n",
        "              'Homogen_scaled': HM_Km_nor_scaled,\n",
        "              'AvgSH_pca2': SH_Km_nor_pca2,\n",
        "              'Homogen_pca2': HM_Km_nor_pca2,\n",
        "              'AvgSH_pca90': SH_Km_nor_pca90,\n",
        "              'Homogen_pca90': HM_Km_nor_pca90\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HFlh136vBdd"
      },
      "source": [
        "## B. Spectral Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZIGPa9dKjYP"
      },
      "source": [
        "from sklearn.cluster import SpectralClustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdXzDftY6pt"
      },
      "source": [
        "#### i. All Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gOJf5GQJzGV"
      },
      "source": [
        "## Spectral iterated over cluster size = 2 through cluster size ##\n",
        "## = 7 for each of the feature sets for the whole sample.      ##\n",
        "\n",
        "# Raw Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_all_raw = []\n",
        "HM_SC_all_raw = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_all)    \n",
        "    SC = SC.fit(X_all)\n",
        "    #lbls = SC.fit_predict(X_all)\n",
        "    #df_all['SC_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_all,  SC.fit_predict(X_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_all_raw.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_all), df_all['hdremit.all']),3)\n",
        "    HM_SC_all_raw.append(hm_score)\n",
        "\n",
        "# Scaled Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_all_scaled = []\n",
        "HM_SC_all_scaled = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_scaled_all)    \n",
        "    SC = SC.fit(X_scaled_all)\n",
        "    #lbls = SC.fit_predict(X_scaled_all)\n",
        "    #df_all['SC_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_scaled_all,  SC.fit_predict(X_scaled_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_all_scaled.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_scaled_all), df_all['hdremit.all']),3)\n",
        "    HM_SC_all_scaled.append(hm_score)\n",
        "        \n",
        "\n",
        "# PCA (n = 2)\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_all_pca2 = []\n",
        "HM_SC_all_pca2 = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_principal2_all)    \n",
        "    SC = SC.fit(X_principal2_all)\n",
        "    lbls = SC.fit_predict(X_principal2_all)  # storing labels for this run because it was selected as optimal feature set\n",
        "    df_all['sc_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal2_all,  SC.fit_predict(X_principal2_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_all_pca2.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_principal2_all), df_all['hdremit.all']),3)\n",
        "    HM_SC_all_pca2.append(hm_score)   \n",
        "\n",
        "# PCA (n=90)     \n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_all_pca90 = []\n",
        "HM_SC_all_pca90 = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_principal90_all)    \n",
        "    SC = SC.fit(X_principal90_all)\n",
        "    #lbls = SC.fit_predict(X_principal90_all)\n",
        "    #df_all['SC_cl_all_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal90_all,  SC.fit_predict(X_principal90_all), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_all_pca90.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_principal90_all), df_all['hdremit.all']),3)\n",
        "    HM_SC_all_pca90.append(hm_score)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQvqXmQJLNFn"
      },
      "source": [
        "# Summary of homogeneity and silhoutte across feature sets in escitlapram sample using SC\n",
        "pd.DataFrame({'ClusterSize' : [2,3,4,5,6,7],\n",
        "              'AvgSH_raw': SH_SC_all_raw,\n",
        "              'Homogen_raw': HM_SC_all_raw,\n",
        "              'AvgSH_scaled': SH_SC_all_scaled,\n",
        "              'Homogen_scaled': HM_SC_all_scaled,\n",
        "              'AvgSH_pca2': SH_SC_all_pca2,\n",
        "              'Homogen_pca2': HM_SC_all_pca2,\n",
        "              'AvgSH_pca90': SH_SC_all_pca90,\n",
        "              'Homogen_pca90': HM_SC_all_pca90\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU4PGYZfmFUN"
      },
      "source": [
        "#### ii. Escitalopram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCm-NR1MMHif"
      },
      "source": [
        "## Spectral iterated over cluster size = 2 through cluster size ##\n",
        "## = 7 for each of the feature sets for the whole sample.      ##\n",
        "\n",
        "# Raw Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_esc_raw = []\n",
        "HM_SC_esc_raw = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_esc)    \n",
        "    SC = SC.fit(X_esc)\n",
        "    #lbls = SC.fit_predict(X_esc)\n",
        "    #df_esc['SC_cl_esc_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_esc,  SC.fit_predict(X_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_esc_raw.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_esc), df_esc['hdremit.all']),3)\n",
        "    HM_SC_esc_raw.append(hm_score)\n",
        "\n",
        "# Scaled Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_esc_scaled = []\n",
        "HM_SC_esc_scaled = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_scaled_esc)    \n",
        "    SC = SC.fit(X_scaled_esc)\n",
        "    #lbls = SC.fit_predict(X_scaled_esc)\n",
        "    #df_esc['SC_cl_esc_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_scaled_esc,  SC.fit_predict(X_scaled_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_esc_scaled.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_scaled_esc), df_esc['hdremit.all']),3)\n",
        "    HM_SC_esc_scaled.append(hm_score)\n",
        "        \n",
        "\n",
        "# PCA (n = 2)\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_esc_pca2 = []\n",
        "HM_SC_esc_pca2 = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_principal2_esc)    \n",
        "    SC = SC.fit(X_principal2_esc)\n",
        "    lbls = SC.fit_predict(X_principal2_esc)  # storing labels for this run because it was selected as optimal feature set\n",
        "    df_esc['sc_cl_esc_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal2_esc,  SC.fit_predict(X_principal2_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_esc_pca2.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_principal2_esc), df_esc['hdremit.all']),3)\n",
        "    HM_SC_esc_pca2.append(hm_score)   \n",
        "\n",
        "# PCA (n=90)     \n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_esc_pca90 = []\n",
        "HM_SC_esc_pca90 = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_principal90_esc)    \n",
        "    SC = SC.fit(X_principal90_esc)\n",
        "    #lbls = SC.fit_predict(X_principal90_esc)\n",
        "    #df_esc['SC_cl_esc_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal90_esc,  SC.fit_predict(X_principal90_esc), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_esc_pca90.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_principal90_esc), df_esc['hdremit.all']),3)\n",
        "    HM_SC_esc_pca90.append(hm_score)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOqFUB7xMXme"
      },
      "source": [
        "# Summary of homogeneity and silhoutte across feature sets in escitlapram sample using SC\n",
        "pd.DataFrame({'ClusterSize' : [2,3,4,5,6,7],\n",
        "              'AvgSH_raw': SH_SC_esc_raw,\n",
        "              'Homogen_ra': HM_SC_esc_raw,\n",
        "              'AvgSH_scaled': SH_SC_esc_scaled,\n",
        "              'Homogen_scaled': HM_SC_esc_scaled,\n",
        "              'AvgSH_pca2': SH_SC_esc_pca2,\n",
        "              'Homogen_pca2': HM_SC_esc_pca2,\n",
        "              'AvgSH_pca90': SH_SC_esc_pca90,\n",
        "              'Homogen_pca90': HM_SC_esc_pca90\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM-htCV_NR92"
      },
      "source": [
        "### iii. Nortryptaline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Y5vtjhNWrl"
      },
      "source": [
        "## Spectral iterated over cluster size = 2 through cluster size ##\n",
        "## = 7 for each of the feature sets for the nortryptaline sample      ##\n",
        "\n",
        "# Raw Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_nor_raw = []\n",
        "HM_SC_nor_raw = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_nor)    \n",
        "    SC = SC.fit(X_nor)\n",
        "    #lbls = SC.fit_predict(X_nor)\n",
        "    #df_nor['SC_cl_nor_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_nor,  SC.fit_predict(X_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_nor_raw.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_nor), df_nor['hdremit.all']),3)\n",
        "    HM_SC_nor_raw.append(hm_score)\n",
        "\n",
        "# Scaled Data\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_nor_scaled = []\n",
        "HM_SC_nor_scaled = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_scaled_nor)    \n",
        "    SC = SC.fit(X_scaled_nor)\n",
        "    #lbls = SC.fit_predict(X_scaled_nor)\n",
        "    #df_nor['SC_cl_nor_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_scaled_nor,  SC.fit_predict(X_scaled_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_nor_scaled.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_scaled_nor), df_nor['hdremit.all']),3)\n",
        "    HM_SC_nor_scaled.append(hm_score)\n",
        "        \n",
        "\n",
        "# PCA (n = 2)\n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_nor_pca2 = []\n",
        "HM_SC_nor_pca2 = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_principal2_nor)    \n",
        "    SC = SC.fit(X_principal2_nor)\n",
        "    lbls = SC.fit_predict(X_principal2_nor)  # storing labels for this run because it was selected as optimal feature set\n",
        "    df_nor['sc_cl_nor_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal2_nor,  SC.fit_predict(X_principal2_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_nor_pca2.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_principal2_nor), df_nor['hdremit.all']),3)\n",
        "    HM_SC_nor_pca2.append(hm_score)   \n",
        "\n",
        "# PCA (n=90)     \n",
        "\n",
        "K = range(2,8,1)\n",
        "SH_SC_nor_pca90 = []\n",
        "HM_SC_nor_pca90 = []\n",
        "for k in K:\n",
        "    SC = SpectralClustering(n_clusters = k, affinity ='nearest_neighbors').fit(X_principal90_nor)    \n",
        "    SC = SC.fit(X_principal90_nor)\n",
        "    #lbls = SC.fit_predict(X_principal90_nor)\n",
        "    #df_nor['SC_cl_nor_' + str(k)] = lbls\n",
        "    sh_score = round(silhouette_score(X_principal90_nor,  SC.fit_predict(X_principal90_nor), metric='euclidean', random_state=random_state),3)\n",
        "    SH_SC_nor_pca90.append(sh_score)\n",
        "    hm_score = round(metrics.homogeneity_score(SC.fit_predict(X_principal90_nor), df_nor['hdremit.all']),3)\n",
        "    HM_SC_nor_pca90.append(hm_score)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_BVAwQVNqY0"
      },
      "source": [
        "# Summary of homogeneity and silhoutte across feature sets in nortyptalin sample using SC\n",
        "pd.DataFrame({'ClusterSize' : [2,3,4,5,6,7],\n",
        "              'AvgSH_raw': SH_SC_nor_raw,\n",
        "              'Homogen_ra': HM_SC_nor_raw,\n",
        "              'AvgSH_scaled': SH_SC_nor_scaled,\n",
        "              'Homogen_scaled': HM_SC_nor_scaled,\n",
        "              'AvgSH_pca2': SH_SC_nor_pca2,\n",
        "              'Homogen_pca2': HM_SC_nor_pca2,\n",
        "              'AvgSH_pca90': SH_SC_nor_pca90,\n",
        "              'Homogen_pca90': HM_SC_nor_pca90\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4su0dHN-Dt"
      },
      "source": [
        "# VI) Plotting Validation Scores by K-size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv96YPJDOEqn"
      },
      "source": [
        "Below are plots for the variation in homogeneity and silhoutte scores as a function of clusters size across each of the samples. These are based on validation scores for the best performing feature set (i.e., PCA n_components = 2).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OVTJhppObIs"
      },
      "source": [
        "## i. All Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0Qqck6zOdV_"
      },
      "source": [
        "## AVG SILHOUTTE SCORE##\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bars\n",
        "barWidth = 0.25\n",
        " \n",
        "# set heights of bars\n",
        "bars1 = SH_Km_all_pca2 \n",
        "bars2 = SH_SC_all_pca2 \n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(r1, bars1, color='green', width=barWidth, edgecolor='white')\n",
        "plt.bar(r2, bars2, color='blue', width=barWidth, edgecolor='white')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Cluster Size', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['k=2', 'k=3', 'k=4', 'k=5', 'k=6','k=7'])\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.title(\"Average Silhoutte Score\")\n",
        "plt.savefig(\"SH_all.png\")\n",
        "plt.show()\n",
        "\n",
        "# save plot\n",
        "#from google.colab import files\n",
        "#files.download(\"SH_all.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8-68iGpQBAL"
      },
      "source": [
        "## HOMOGENEITY ##\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bars\n",
        "barWidth = 0.25\n",
        " \n",
        "# set heights of bars\n",
        "bars1 = HM_Km_all_pca2\n",
        "bars2 = HM_SC_all_pca2\n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(r1, bars1, color='green', width=barWidth, edgecolor='white', label='K-means Clustering')\n",
        "plt.bar(r2, bars2, color='blue', width=barWidth, edgecolor='white', label = 'Spectral Clustering')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Cluster Size', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['k=2', 'k=3', 'k=4', 'k=5', 'k=6','k=7'])\n",
        "plt.title(\"Homogeneity\")\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.legend(loc= 'upper right', ncol = 2)\n",
        "plt.savefig(\"HM_all.png\")\n",
        "plt.show()\n",
        "\n",
        "# save plot\n",
        "# from google.colab import files\n",
        "# files.download(\"HM_all.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgRtoDdEQKT-"
      },
      "source": [
        "## ii. Escitalopram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfaFpS1mQM6K"
      },
      "source": [
        "## AVG SILHOUTTE SCORE##\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bars\n",
        "barWidth = 0.25\n",
        " \n",
        "# set heights of bars\n",
        "bars1 = SH_Km_esc_pca2\n",
        "bars2 = SH_SC_esc_pca2\n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(r1, bars1, color='green', width=barWidth, edgecolor='white')\n",
        "plt.bar(r2, bars2, color='blue', width=barWidth, edgecolor='white')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Cluster Size', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['k=2', 'k=3', 'k=4', 'k=5', 'k=6','k=7'])\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.savefig(\"SH_esc.png\")\n",
        "plt.show()\n",
        "\n",
        "# save plot\n",
        "from google.colab import files\n",
        "files.download(\"SH_esc.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFS7khBrQdOr"
      },
      "source": [
        "## HOMOGENEITY ##\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bars\n",
        "barWidth = 0.25\n",
        " \n",
        "# set heights of bars\n",
        "bars1 = HM_Km_esc_pca2\n",
        "bars2 = HM_SC_esc_pca2\n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(r1, bars1, color='green', width=barWidth, edgecolor='white')\n",
        "plt.bar(r2, bars2, color='blue', width=barWidth, edgecolor='white')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Cluster Size', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['k=2', 'k=3', 'k=4', 'k=5', 'k=6','k=7'])\n",
        "\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.savefig(\"HM_ESC.png\")\n",
        "plt.show()\n",
        "\n",
        "# save plot\n",
        "from google.colab import files\n",
        "files.download(\"HM_ESC.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gaw_J86QhzR"
      },
      "source": [
        "## iii. Nortryptaline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mA_BCa6Qk6x"
      },
      "source": [
        "## AVG SILHOUTTE SCORE##\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bars\n",
        "barWidth = 0.25\n",
        " \n",
        "# set heights of bars\n",
        "bars1 = SH_Km_nor_pca2\n",
        "bars2 = SH_SC_nor_pca2\n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(r1, bars1, color='green', width=barWidth, edgecolor='white')\n",
        "plt.bar(r2, bars2, color='blue', width=barWidth, edgecolor='white')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Cluster Size', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['k=2', 'k=3', 'k=4', 'k=5', 'k=6','k=7'])\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.savefig(\"SH_nor.png\")\n",
        "plt.show()\n",
        "\n",
        "# save plot\n",
        "from google.colab import files\n",
        "files.download(\"SH_nor.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58WmkGrKQwx0"
      },
      "source": [
        "## HOMOGENEITY ##\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bars\n",
        "barWidth = 0.25\n",
        " \n",
        "# set heights of bars\n",
        "bars1 = HM_Km_nor_pca2\n",
        "bars2 = HM_SC_nor_pca2\n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        " \n",
        "# Make the plot\n",
        "plt.bar(r1, bars1, color='green', width=barWidth, edgecolor='white')\n",
        "plt.bar(r2, bars2, color='blue', width=barWidth, edgecolor='white')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Cluster Size', fontweight='bold')\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], ['k=2', 'k=3', 'k=4', 'k=5', 'k=6','k=7'])\n",
        "\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "plt.savefig(\"HM_nor.png\")\n",
        "plt.show()\n",
        "\n",
        "# save plot\n",
        "# from google.colab import files\n",
        "# files.download(\"HM_nor.png\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxkn5YT4UXJj"
      },
      "source": [
        "# VI) Prediction Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M24B0BL9XmnY"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import cross_validate\n",
        "from numpy import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKuwSkZyUgCV"
      },
      "source": [
        "## i. K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIoxipcaWa5I"
      },
      "source": [
        "### a. All Sample\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew00ey-aSorC"
      },
      "source": [
        "#### ALL SAMPLE w/o cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDdDMPz-SorD"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRqjMN7dSorD"
      },
      "source": [
        "X_sup_all = df_all.drop([\"km_cl_all_2\",\"km_cl_all_3\",\"km_cl_all_4\", \"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\"sc_cl_all_7\"],axis=1)\n",
        "Y_sup_all = df_all[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWQNQcR-SorD"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_all)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuktP1YESorD"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufMlkjPjSorD"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_all = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_result = cross_validate(xgbrndm_all,X_sup_all,Y_sup_all,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNj-8IywSorD"
      },
      "source": [
        "cv_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cowymWI8SorE"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_result['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_result['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision =  {round(mean(cv_result['test_precision']),3)}\")\n",
        "print(f\"Mean Recall =  {round(mean(cv_result['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHuPTNKJF1UM"
      },
      "source": [
        "#### K = 2 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0eeVjuXto0P"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZEzDvQXth2t"
      },
      "source": [
        "X_sup_all_k2 = df_all.drop([\"km_cl_all_3\",\"km_cl_all_4\", \"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\"sc_cl_all_7\"],axis=1)\n",
        "Y_sup_all_k2 = df_all[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBD_KsQe2hkc"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_all_k2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb9FDG4t2hkc"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q01ycVJ6181l"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_all_k2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results3 = cross_validate(xgbrndm_all_k2,X_sup_all_k2,Y_sup_all_k2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEac1nFzotYi"
      },
      "source": [
        "cv_results3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBdAhqYuwzWC"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results3['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision =  {round(mean(cv_results3['test_precision']),3)}\")\n",
        "print(f\"Mean Recall =  {round(mean(cv_results3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1-X1lVc2nJS"
      },
      "source": [
        "#### K = 2 w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP-v4Gjj2w1U"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McL1xTwC2fMI"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56dQJZa92fMI"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km2_all_cl1 = df_all.loc[df_all['km_cl_all_2'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km2_all_cl1 = X_km2_all_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km2_all_cl1 = X_km2_all_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km2_all_cl1 = X_km2_all_cl1.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHPigM9O2fMI"
      },
      "source": [
        "if len(Y_km2_all_cl1) == len(X_km2_all_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "len(X_km2_all_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WawryJj32fMJ"
      },
      "source": [
        "X_km2_all_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfF_5C6B2fMJ"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km2_all_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gsOjcNt2fMJ"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km2_all_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXUQGisJ2fMJ"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(2.52,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wVhRqnF2fMJ"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=random_state)\n",
        "xgbrndm_all_k2_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_k2_cl1 = cross_validate(xgbrndm_all_k2_cl1,X_km2_all_cl1,Y_km2_all_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emY42lL-2fMJ"
      },
      "source": [
        "cvresults_all_k2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suv5lVsE2fMK"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_k2_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_k2_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_k2_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_k2_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgm4YHSP2fMK"
      },
      "source": [
        "m_all_k2_cl1 = xgbrndm_all_k2_cl1.fit(X_km2_all_cl1,Y_km2_all_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVGgDHXW2fMO"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfSBpQbx2fMO"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_k2_cl1 = pd.DataFrame({'fscore':m_all_k2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km2_all_cl1)})\n",
        "importances_all_k2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEeGUhWH4B_m"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWWombgb4B_n"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8JLaccm4B_n"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_km2_all_cl0 = df_all.loc[df_all['km_cl_all_2'] == 0]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km2_all_cl0 = X_km2_all_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km2_all_cl0 = X_km2_all_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km2_all_cl0 = X_km2_all_cl0.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWQvxspn4B_o"
      },
      "source": [
        "if len(Y_km2_all_cl0) == len(X_km2_all_cl0):\n",
        "  print(f\"Same Length (n={len(X_km2_all_cl0)})\")\n",
        "else:\n",
        "\n",
        "  print(\"Not Mathcing\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH2MgPdX4B_o"
      },
      "source": [
        "X_km2_all_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzTmLQOr4B_p"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km2_all_cl0.value_counts(normalize=True)\n",
        "##Only slightly imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6CtRoeq4B_p"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km2_all_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "999l82yM4B_p"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjhfXxW14B_q"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_k2_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_k2_cl0 = cross_validate(xgbrndm_all_k2_cl0, X_km2_all_cl0, Y_km2_all_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xICygnFi4B_q"
      },
      "source": [
        "cvresults_all_k2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNH2plZa4B_q"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_k2_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_k2_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_k2_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_k2_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujtna7TB4B_r"
      },
      "source": [
        "m_all_k2_cl0 = xgbrndm_all_k2_cl0.fit(X_km2_all_cl0,Y_km2_all_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01gFVYIY4B_r"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giflv5Mk4B_r"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_k2_cl0 = pd.DataFrame({'fscore':m_all_k2_cl0.best_estimator_.feature_importances_,\n",
        "                                       'varname': list(X_km2_all_cl0)})\n",
        "importances_all_k2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GEAUU80yNhm"
      },
      "source": [
        "#### K = 3 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-cPMigGyNhn"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xFsd98iyNhn"
      },
      "source": [
        "X_sup_all_k3 = df_all.drop([\"km_cl_all_2\",\"km_cl_all_4\", \"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\"sc_cl_all_7\"],axis=1)\n",
        "Y_sup_all_k3 = df_all[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqqt5NpmyVpL"
      },
      "source": [
        "if len(Y_sup_all_k3) == len(X_sup_all_k3):\n",
        "  print(f\"Same Length (n={len(X_sup_all_k3)})\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWCmbTf1yuo_"
      },
      "source": [
        "X_sup_all_k3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upVM_x20zE35"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sup_all_k3.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GrMjOEwyNho"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_all_k3)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfZjvqpyNho"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkLjuAF5yNho"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_k3 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_all_k3 = cross_validate(xgbrndm_all_k3,X_sup_all_k3,Y_sup_all_k3,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZB54AKWyNhp"
      },
      "source": [
        "cv_results_all_k3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3foVosByNhp"
      },
      "source": [
        "from numpy import mean\n",
        "print(f\"Mean Accuracy = {round(mean(cv_results_all_k3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_all_k3['test_roc_auc']),4)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_all_k3['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cv_results_all_k3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVCLbA8yNhq"
      },
      "source": [
        "m_all_k3 = xgbrndm_all_k3.fit(X_sup_all_k3,Y_sup_all_k3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWX9V6beyNhq"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIY9Ka_vyNhr"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_k3 = pd.DataFrame({'fscore':m_all_k3.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sup_all_k3)})\n",
        "importances_all_k3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07MPEPlENN_5"
      },
      "source": [
        "#### K = 3; w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfKMY2YOM8TJ"
      },
      "source": [
        "##### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-xfMP1OM8TN"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srZS27JcM8TN"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km3_all_cl2 = df_all.loc[df_all['km_cl_all_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km3_all_cl2 = X_km3_all_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_all_cl2 = X_km3_all_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_all_cl2 = X_km3_all_cl2.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwj5uWlMM8TO"
      },
      "source": [
        "if len(Y_km3_all_cl2) == len(X_km3_all_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_km3_all_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2-DDeB2M8TO"
      },
      "source": [
        "X_km3_all_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVXTdTjFM8TO"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_all_cl2.value_counts(normalize=True)\n",
        "## Slightly Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_gtoX8DM8TO"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_all_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap3HFqwPM8TO"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.2,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3t2jtHnM8TP"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_k3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_k3_cl2 = cross_validate(xgbrndm_all_k3_cl2,X_km3_all_cl2,Y_km3_all_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wzmJlVPM8TP"
      },
      "source": [
        "cvresults_all_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWFkpBQnM8TP"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_k3_cl2['test_accuracy']),4)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_k3_cl2['test_roc_auc']),4)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_k3_cl2['test_precision']),4)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_k3_cl2['test_recall']),4)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92IHMUzfM8TP"
      },
      "source": [
        "m_all_k3_cl2 = xgbrndm_all_k3_cl2.fit(X_km3_all_cl2,Y_km3_all_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aln1g-UpM8TP"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBRiKBAYM8TP"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_k3_cl2 = pd.DataFrame({'fscore':m_all_k3_cl2.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_all_cl2)})\n",
        "importances_all_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptdcofutPi6f"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9CXCsDXPefk"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENctQNP-Pefk"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km3_all_cl1 = df_all.loc[df_all['km_cl_all_3'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km3_all_cl1 = X_km3_all_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_all_cl1 = X_km3_all_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_all_cl1 = X_km3_all_cl1.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3fDYvavPefl"
      },
      "source": [
        "if len(Y_km3_all_cl1) == len(X_km3_all_cl1):\n",
        "  print(f\"Same Length {len(Y_km3_all_cl1)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_km3_all_cl1)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-CQS8lhPefl"
      },
      "source": [
        "X_km3_all_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSdi6GACPefl"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_all_cl1.value_counts(normalize=True)\n",
        "## Roughly Balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A52dnUnnPefl"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_all_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD-fAIbAPefl"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eInNUxIQPefo"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_k3_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_k3_cl1 = cross_validate(xgbrndm_all_k3_cl1,X_km3_all_cl1,Y_km3_all_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfX8QkLRPefp"
      },
      "source": [
        "cvresults_all_k3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ctl6OyPefp"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_k3_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_k3_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_k3_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_k3_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsHNn4PcPefp"
      },
      "source": [
        "m_all_k3_cl1 = xgbrndm_all_k3_cl1.fit(X_km3_all_cl1,Y_km3_all_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgSownwPefp"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-HU0adnPefp"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_k3_cl1 = pd.DataFrame({'fscore':m_all_k3_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_all_cl1)})\n",
        "importances_all_k3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4zjLIBVXg-b"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DwyYJYQXg-b"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDd_Dk84Xg-c"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_km3_all_cl0 = df_all.loc[df_all['km_cl_all_3'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_km3_all_cl0 = X_km3_all_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_all_cl0 = X_km3_all_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_all_cl0 = X_km3_all_cl0.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx4zjqUlXg-c"
      },
      "source": [
        "if len(Y_km3_all_cl0) == len(X_km3_all_cl0):\n",
        "  print(f\"Same Length {len(Y_km3_all_cl0)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_km3_all_cl0)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6XmYhbxXg-c"
      },
      "source": [
        "X_km3_all_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukUXKPA0Xg-c"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_all_cl0.value_counts(normalize=True)\n",
        "##Very Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-A-VS2zXg-c"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_all_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqs0R2Z_Xg-c"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(3.27,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpQviZfNXg-d"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_k3_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_k3_cl0 = cross_validate(xgbrndm_all_k3_cl0,X_km3_all_cl0,Y_km3_all_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_DBySjsXg-d"
      },
      "source": [
        "cvresults_all_k3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq8ElXyjXg-d"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_k3_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_k3_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_k3_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_k3_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9T1FDxpXg-d"
      },
      "source": [
        "m_all_k3_cl0 = xgbrndm_all_k3_cl0.fit(X_km3_all_cl0,Y_km3_all_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPxoWipOXg-d"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc3zcMagXg-d"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_k3_cl0 = pd.DataFrame({'fscore':m_all_k3_cl0.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_all_cl0)})\n",
        "importances_all_k3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvgZ5v9toadD"
      },
      "source": [
        "### b. Escitalopram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiioFsIg1Xo"
      },
      "source": [
        "#### ESCITALOPRAM ; w/o cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_lwqoSng1Xp"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n324ZNDg1Xp"
      },
      "source": [
        "X_sup_esc = df_esc.drop([\"km_cl_esc_2\",\"km_cl_esc_3\",\"km_cl_esc_4\", \"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\"sc_cl_esc_7\"],axis=1)\n",
        "Y_sup_esc = df_esc[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gxf7JU2g1Xp"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_esc)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcqYCQptg1Xq"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHG11r3ag1Xq"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_esc = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_result_esc = cross_validate(xgbrndm_esc,X_sup_esc,Y_sup_esc,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhQtOtgRg1Xq"
      },
      "source": [
        "cv_result_esc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVfgk2dPg1Xr"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_result_esc['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_result_esc['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision =  {round(mean(cv_result_esc['test_precision']),3)}\")\n",
        "print(f\"Mean Recall =  {round(mean(cv_result_esc['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sogj7_SAe6Io"
      },
      "source": [
        "#### K=2; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxbuuCKtfOaM"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEF9GerjfOaN"
      },
      "source": [
        "X_sup_esc_k2 = df_esc.drop([\"km_cl_esc_3\",\"km_cl_esc_4\", \"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\"sc_cl_esc_7\"],axis=1)\n",
        "Y_sup_esc_k2 = df_esc[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHWTeRe0hCg_"
      },
      "source": [
        "print(len(X_sup_esc_k2))\n",
        "print(len(Y_sup_esc_k2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcqvkJVj0CEX"
      },
      "source": [
        "Y_sup_esc_k2.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ggolt7XfOaN"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_esc_k2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AodROKapfOaN"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV15hsKCfOaN"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_esc_k2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_esc_k2 = cross_validate(xgbrndm_esc_k2,X_sup_esc_k2,Y_sup_esc_k2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRY5RuJQfOaO"
      },
      "source": [
        "cv_results_esc_k2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE7A8L63fOaO"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_esc_k2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_esc_k2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_esc_k2['test_precision']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_esc_k2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y04ddEK_fOaO"
      },
      "source": [
        "m_esc_k2 = xgbrndm_esc_k2.fit(X_sup_esc_k2,Y_sup_esc_k2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-xkQfYTfOaO"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqqlb7dgfOaO"
      },
      "source": [
        "# Feature importance\n",
        "importance_esc_k2 = pd.DataFrame({'fscore':m_esc_k2.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_esc_k2)})\n",
        "importance_esc_k2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOfqqzHniQux"
      },
      "source": [
        "#### K=3; w/ cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDmj8ffoigeO"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XffWVjkcigeO"
      },
      "source": [
        "X_sup_esc_k3 = df_esc.drop([\"km_cl_esc_2\",\"km_cl_esc_4\", \"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\"sc_cl_esc_7\"],axis=1)\n",
        "Y_sup_esc_k3 = df_esc[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rd-_5xwigeP"
      },
      "source": [
        "print(len(X_sup_esc_k3))\n",
        "print(len(Y_sup_esc_k3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kotvijv2zvee"
      },
      "source": [
        "Y_sup_esc_k3.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ps1MVjtigeP"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_esc_k3)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JqUgwksigeP"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.36,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQmZbgPigeP"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_esc_k3 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_esc_k3 = cross_validate(xgbrndm_esc_k3,X_sup_esc_k3,Y_sup_esc_k3,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFEb_w_0igeP"
      },
      "source": [
        "cv_results_esc_k3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz3hzvAMigeQ"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_esc_k3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_esc_k3['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_esc_k3['test_precision']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_esc_k3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv__Fb2digeQ"
      },
      "source": [
        "m_esc_k3 = xgbrndm_esc_k3.fit(X_sup_esc_k3,Y_sup_esc_k3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gElqJuovigeQ"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYxMkmf6igeQ"
      },
      "source": [
        "# Feature importance\n",
        "importance_esc_k3 = pd.DataFrame({'fscore':m_esc_k3.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_esc_k3)})\n",
        "importance_esc_k3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vLqh_5akhhh"
      },
      "source": [
        "#### K = 2; w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIf5xOgokhhh"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXYiPcn3khhh"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp7m848fkhhi"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km2_esc_cl1 = df_esc.loc[df_esc['km_cl_esc_2'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km2_esc_cl1 = X_km2_esc_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km2_esc_cl1 = X_km2_esc_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km2_esc_cl1 = X_km2_esc_cl1.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya60Z8h6khhi"
      },
      "source": [
        "if len(Y_km2_esc_cl1) == len(X_km2_esc_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_km2_esc_cl1))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5YSYvJNkhhi"
      },
      "source": [
        "X_km2_esc_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a8lDz19khhj"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km2_esc_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsSCVA9bkhhj"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km2_esc_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ZVc-_Fkhhj"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb_SX371khhj"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_k2_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_k2_cl1 = cross_validate(xgbrndm_esc_k2_cl1,X_km2_esc_cl1,Y_km2_esc_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1TU69RFkhhk"
      },
      "source": [
        "cvresults_esc_k2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv0xj0sikhhk"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_k2_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_k2_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_k2_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_esc_k2_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec09BhDJkhhk"
      },
      "source": [
        "m_esc_k2_cl1 = xgbrndm_esc_k2_cl1.fit(X_km2_esc_cl1,Y_km2_esc_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gHweUjhkhhk"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwC3PqPfkhhl"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_k2_cl1 = pd.DataFrame({'fscore':m_esc_k2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km2_esc_cl1)})\n",
        "importances_esc_k2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leGLSgnRlara"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyGWcLrSlara"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjXG-Cwqlara"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_km2_esc_cl0 = df_esc.loc[df_esc['km_cl_esc_2'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_km2_esc_cl0 = X_km2_esc_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km2_esc_cl0 = X_km2_esc_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km2_esc_cl0 = X_km2_esc_cl0.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jZNaC5Dlarb"
      },
      "source": [
        "if len(Y_km2_esc_cl0) == len(X_km2_esc_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U03Y0a3flarb"
      },
      "source": [
        "X_km2_esc_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J07QZsrmlarb"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km2_esc_cl0.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrwSetZ3larb"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km2_esc_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CZxlrOhlarc"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(2.19,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U-ZAEPVlarc"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_k2_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_k2_cl0 = cross_validate(xgbrndm_esc_k2_cl0,X_km2_esc_cl0,Y_km2_esc_cl0,cv=5,\n",
        "                                      scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiFyIeMhlarc"
      },
      "source": [
        "cvresults_esc_k2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfBhR0Solarc"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_k2_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_k2_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_k2_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_esc_k2_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc1BGPzblard"
      },
      "source": [
        "m_esc_k2_cl0 = xgbrndm_esc_k2_cl0.fit(X_km2_esc_cl0,Y_km2_esc_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-UqW_9flard"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Hsgm44lard"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_k2_cl0 = pd.DataFrame({'fscore':m_esc_k2_cl0.best_estimator_.feature_importances_,\n",
        "                                      'varname': list(X_km2_esc_cl0)})\n",
        "importances_esc_k2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "docWTLKSmVwE"
      },
      "source": [
        "#### K = 3; w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKN0juPxr-0h"
      },
      "source": [
        "##### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XWeMSlfr-00"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcFAgqJdr-01"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_km3_esc_cl2 = df_esc.loc[df_esc['km_cl_esc_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 2:\n",
        "Y_km3_esc_cl2 = X_km3_esc_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_esc_cl2 = X_km3_esc_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_esc_cl2 = X_km3_esc_cl2.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPOSeRtbr-01"
      },
      "source": [
        "if len(Y_km3_esc_cl2) == len(X_km3_esc_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Matching\")\n",
        "\n",
        "\n",
        "print(len(Y_km3_esc_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obVkYuOjr-01"
      },
      "source": [
        "X_km3_esc_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O-xWfXvr-02"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_esc_cl2.value_counts(normalize=True)\n",
        "##rouhgly balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tubHENCjr-02"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_esc_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHaLHylKr-03"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMmJnGzFr-03"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_k3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_k3_cl2 = cross_validate(xgbrndm_esc_k3_cl2,X_km3_esc_cl2,Y_km3_esc_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-6LKBxQr-04"
      },
      "source": [
        "cvresults_esc_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inJC6kTur-04"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_k3_cl2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_k3_cl2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_k3_cl2['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_esc_k3_cl2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiKcaXWEr-04"
      },
      "source": [
        "m_esc_k3_cl2 = xgbrndm_esc_k3_cl2.fit(X_km3_esc_cl2,Y_km3_esc_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOKbC5ZIr-05"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLvCKCGir-05"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_k3_cl2 = pd.DataFrame({'fscore':m_esc_k3_cl2.best_estimator_.feature_importances_,\n",
        "                                       'varname': list(X_km3_esc_cl2)})\n",
        "importances_esc_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNHWF1A2mVwH"
      },
      "source": [
        "Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VouB3pIXmVwI"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVfsUU2-mVwI"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_km3_esc_cl2 = df_esc.loc[df_esc['km_cl_esc_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km3_esc_cl2 = X_km3_esc_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_esc_cl2 = X_km3_esc_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_esc_cl2 = X_km3_esc_cl2.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPk8yYX4mVwJ"
      },
      "source": [
        "if len(Y_km3_esc_cl2) == len(X_km3_esc_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "\n",
        "print(len(Y_km3_esc_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p1JpjdrmVwJ"
      },
      "source": [
        "X_km3_esc_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U2QdmZtmVwJ"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_esc_cl2.value_counts(normalize=True)\n",
        "##rouhgly balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsXlRXp9mVwJ"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_esc_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYmCAMWumVwJ"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(0.72,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWQJdAeDmVwK"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_k3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_k3_cl2 = cross_validate(xgbrndm_esc_k3_cl2,X_km3_esc_cl2,Y_km3_esc_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2_t3XnKmVwK"
      },
      "source": [
        "cvresults_esc_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRZL5mw7mVwK"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_k3_cl2['test_accuracy']),2)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_k3_cl2['test_roc_auc']),2)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_k3_cl2['test_precision']),2)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_esc_k3_cl2['test_recall']),2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KIHz92MmVwK"
      },
      "source": [
        "m_esc_k3_cl2 = xgbrndm_esc_k3_cl2.fit(X_km3_esc_cl2,Y_km3_esc_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaH5slDEmVwK"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W11sqJ-JmVwL"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_k3_cl2 = pd.DataFrame({'fscore':m_esc_k3_cl2.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_esc_cl2)})\n",
        "importances_esc_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSmA6f-nsekr"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv4kfasisekr"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSmzQCkhseks"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km3_esc_cl1 = df_esc.loc[df_esc['km_cl_esc_3'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km3_esc_cl1 = X_km3_esc_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_esc_cl1 = X_km3_esc_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_esc_cl1 = X_km3_esc_cl1.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIRAeRPCseks"
      },
      "source": [
        "if len(Y_km3_esc_cl1) == len(X_km3_esc_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Matching\")\n",
        "\n",
        "\n",
        "print(len(Y_km3_esc_cl1))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1tFrkF5seks"
      },
      "source": [
        "X_km3_esc_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLRhGmjqseks"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_esc_cl1.value_counts(normalize=True)\n",
        "##rouhgly balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlfHYkSUseks"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_esc_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hrd4VcRsekt"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.23,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XlbH9Y4sekt"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_k3_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_k3_cl1 = cross_validate(xgbrndm_esc_k3_cl1,X_km3_esc_cl1,Y_km3_esc_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36uf1uB8sekt"
      },
      "source": [
        "cvresults_esc_k3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ju-ffLsekt"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_k3_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_k3_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_k3_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_esc_k3_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYMz1lA_seku"
      },
      "source": [
        "m_esc_k3_cl1 = xgbrndm_esc_k3_cl1.fit(X_km3_esc_cl1,Y_km3_esc_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFINqk66seku"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsdZAeRWseku"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_k3_cl1 = pd.DataFrame({'fscore':m_esc_k3_cl1.best_estimator_.feature_importances_,\n",
        "                                       'varname': list(X_km3_esc_cl1)})\n",
        "importances_esc_k3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgP1bGhbvEP5"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2bguOACvEP6"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5JmU4TZvEP6"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_km3_esc_cl0 = df_esc.loc[df_esc['km_cl_esc_3'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_km3_esc_cl0 = X_km3_esc_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_esc_cl0 = X_km3_esc_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_esc_cl0 = X_km3_esc_cl0.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0pZ_UcZvEP7"
      },
      "source": [
        "if len(Y_km3_esc_cl0) == len(X_km3_esc_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "\n",
        "print(len(Y_km3_esc_cl0))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozG6vI8YvEP7"
      },
      "source": [
        "X_km3_esc_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9xt5r8RvEP7"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_esc_cl0.value_counts(normalize=True)\n",
        "##Very  imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9A5Zlr6vEP8"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_esc_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmx81gCRvEP8"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(3.13,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFeFm6a-vEP8"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_k3_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_k3_cl0 = cross_validate(xgbrndm_esc_k3_cl0,X_km3_esc_cl0,Y_km3_esc_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxfGBHxEvEP8"
      },
      "source": [
        "cvresults_esc_k3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kM-O4kTvEP8"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_k3_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_k3_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_k3_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_esc_k3_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0CIrA1ivEP8"
      },
      "source": [
        "m_esc_k3_cl0 = xgbrndm_esc_k3_cl0.fit(X_km3_esc_cl0,Y_km3_esc_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VYyJ7VRvEP9"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDTk6z2XvEP9"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_k3_cl0 = pd.DataFrame({'fscore':m_esc_k3_cl0.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_esc_cl0)})\n",
        "importances_esc_k3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lDhYCKmmzTa"
      },
      "source": [
        "### c. Nortryptaline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yxwvKfSkWnw"
      },
      "source": [
        "##### NORTRYPTALYINE ; w/o cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhwKTA_DkWnx"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDmuWEDnkWnx"
      },
      "source": [
        "X_sup_nor = df_nor.drop([\"km_cl_nor_2\",\"km_cl_nor_3\",\"km_cl_nor_4\", \"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\"sc_cl_nor_7\"],axis=1)\n",
        "Y_sup_nor = df_nor[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRog5G7zkWnx"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_nor)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS6gtPqkkWnx"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8d2E4xukWnx"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_nor = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_result_nor = cross_validate(xgbrndm_nor,X_sup_nor,Y_sup_nor,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5KQ244KkWnx"
      },
      "source": [
        "cv_result_nor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bnOhSPykWnx"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_result_nor['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_result_nor['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision =  {round(mean(cv_result_nor['test_precision']),3)}\")\n",
        "print(f\"Mean Recall =  {round(mean(cv_result_nor['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjW6aFEd6GQ_"
      },
      "source": [
        "##### NORTRYPTALINE: K-Means K=2; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ1c0CB06GQ_"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmPsPcrO6GRA"
      },
      "source": [
        "X_sup_nor_k2 = df_nor.drop([\"km_cl_nor_3\",\"km_cl_nor_4\", \"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\"sc_cl_nor_7\"],axis=1)\n",
        "Y_sup_nor_k2 = df_nor[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz-5Olkg6GRA"
      },
      "source": [
        "print(len(X_sup_nor_k2))\n",
        "print(len(Y_sup_nor_k2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KU6RqsP6GRA"
      },
      "source": [
        "Y_sup_nor_k2.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6oWTvyl6GRA"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_nor_k2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAwaNncL6GRA"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.9,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGXMaJA6GRA"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_nor_k2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_nor_k2 = cross_validate(xgbrndm_nor_k2,X_sup_nor_k2,Y_sup_nor_k2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQp3UKax6GRA"
      },
      "source": [
        "cv_results_nor_k2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVh55dhX6GRB"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_nor_k2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_nor_k2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_nor_k2['test_precision']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_nor_k2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm4BFsLe6GRB"
      },
      "source": [
        "m_nor_k2 = xgbrndm_nor_k2.fit(X_sup_nor_k2,Y_sup_nor_k2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqZiIJmY6GRB"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLuqJQPb6GRB"
      },
      "source": [
        "# Feature importance\n",
        "importance_nor_k2 = pd.DataFrame({'fscore':m_nor_k2.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_nor_k2)})\n",
        "importance_nor_k2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oISKFSXK6GRB"
      },
      "source": [
        "##### NORTRYPTALINE: K-Means K=3; w/ cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGEBFlrl6GRB"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2yg-Cws6GRB"
      },
      "source": [
        "X_sup_nor_k3 = df_nor.drop([\"km_cl_nor_2\",\"km_cl_nor_4\", \"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\"sc_cl_nor_7\"],axis=1)\n",
        "Y_sup_nor_k3 = df_nor[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olu5NdWj6GRB"
      },
      "source": [
        "print(len(X_sup_nor_k3))\n",
        "print(len(Y_sup_nor_k3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Te6uIX36GRB"
      },
      "source": [
        "Y_sup_nor_k3.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYazLu086GRC"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_nor_k3)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj3NpLWn6GRC"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.92,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPgfc49f6GRC"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_nor_k3 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_nor_k3 = cross_validate(xgbrndm_nor_k3,X_sup_nor_k3,Y_sup_nor_k3,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7rWUvu96GRC"
      },
      "source": [
        "cv_results_nor_k3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M7eNP9x6GRC"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_nor_k3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_nor_k3['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_nor_k3['test_precision']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_nor_k3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LUYulpu6GRC"
      },
      "source": [
        "m_nor_k3 = xgbrndm_nor_k3.fit(X_sup_nor_k3,Y_sup_nor_k3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1vYgxFU6GRC"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9MBw1aL6GRC"
      },
      "source": [
        "# Feature importance\n",
        "importance_nor_k3 = pd.DataFrame({'fscore':m_nor_k3.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_nor_k3)})\n",
        "importance_nor_k3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtxWZHY8JAoy"
      },
      "source": [
        "##### NORTRYPTALINE: K-Means K = 2 w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WV2zupJAoz"
      },
      "source": [
        "###### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt6brHMTJAo0"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCneEniXJAo0"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km2_nor_cl1 = df_nor.loc[df_nor['km_cl_nor_2'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km2_nor_cl1 = X_km2_nor_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km2_nor_cl1 = X_km2_nor_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km2_nor_cl1 = X_km2_nor_cl1.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T0UDuKuJAo0"
      },
      "source": [
        "if len(Y_km2_nor_cl1) == len(X_km2_nor_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_km2_nor_cl1))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-y2Em7TJAo0"
      },
      "source": [
        "X_km2_nor_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRJvXTn6JAo1"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km2_nor_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsXmuy9dJAo1"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km2_nor_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjp2RpH2JAo1"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(2.87,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6JqRsvvJAo1"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_k2_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_k2_cl1 = cross_validate(xgbrndm_nor_k2_cl1,X_km2_nor_cl1,Y_km2_nor_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgvoNkXOJAo2"
      },
      "source": [
        "cvresults_nor_k2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exCO6alcJAo2"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_k2_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_k2_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_k2_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_k2_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QI4_WlxJAo6"
      },
      "source": [
        "m_nor_k2_cl1 = xgbrndm_nor_k2_cl1.fit(X_km2_nor_cl1,Y_km2_nor_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVyl0zF3JAo6"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZpwQE_UJAo6"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_k2_cl1 = pd.DataFrame({'fscore':m_nor_k2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km2_nor_cl1)})\n",
        "importances_nor_k2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w53xM8llN_Qz"
      },
      "source": [
        "###### Cluster 0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1HbnmQMN_Qz"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gx3ii0KN_Q0"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_km2_nor_cl0 = df_nor.loc[df_nor['km_cl_nor_2'] ==0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_km2_nor_cl0 = X_km2_nor_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km2_nor_cl0 = X_km2_nor_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km2_nor_cl0 = X_km2_nor_cl0.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zl_VqK9N_Q0"
      },
      "source": [
        "if len(Y_km2_nor_cl0) == len(X_km2_nor_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(X_km2_nor_cl0))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7C2D75zN_Q0"
      },
      "source": [
        "X_km2_nor_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrGUWhTUN_Q1"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km2_nor_cl0.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X76fagw9N_Q1"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km2_nor_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC5EPeoqN_Q1"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.45,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G1g_QwhN_Q2"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_k2_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_k2_cl0 = cross_validate(xgbrndm_nor_k2_cl0,X_km2_nor_cl0,Y_km2_nor_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp9Twee2N_Q2"
      },
      "source": [
        "cvresults_nor_k2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEFa15rqN_Q2"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_k2_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_k2_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_k2_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_k2_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2aRsZYzN_Q2"
      },
      "source": [
        "m_nor_k2_cl0 = xgbrndm_nor_k2_cl0.fit(X_km2_nor_cl0,Y_km2_nor_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItbUW2HqN_Q2"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbqIz6fcN_Q3"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_k2_cl0 = pd.DataFrame({'fscore':m_nor_k2_cl0.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km2_nor_cl0)})\n",
        "importances_nor_k2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3SoFJnqdHd"
      },
      "source": [
        "##### NORTRYPTALINE: K-Means K = 3 w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIEVbJv3qdHe"
      },
      "source": [
        "###### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhMkWqGKqdHe"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwlL8HBgqdHf"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_km3_nor_cl2 = df_nor.loc[df_nor['km_cl_nor_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 2:\n",
        "Y_km3_nor_cl2 = X_km3_nor_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_nor_cl2 = X_km3_nor_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_nor_cl2 = X_km3_nor_cl2.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYYefBxUqdHf"
      },
      "source": [
        "if len(Y_km3_nor_cl2) == len(X_km3_nor_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_km3_nor_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYgQ2DJSqdHg"
      },
      "source": [
        "X_km3_nor_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ_nOJQIqdHg"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_nor_cl2.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjszswAAqdHg"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_nor_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x152QcqTqdHh"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.44,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxS3FsZBqdHh"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_k3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_k3_cl2 = cross_validate(xgbrndm_nor_k3_cl2,X_km3_nor_cl2,Y_km3_nor_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2aD89wVqdHh"
      },
      "source": [
        "cvresults_nor_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxHxKBLdqdHi"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_k3_cl2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_k3_cl2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_k3_cl2['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_k3_cl2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoKhsoQYqdHi"
      },
      "source": [
        "m_nor_k3_cl2 = xgbrndm_nor_k3_cl2.fit(X_km3_nor_cl2,Y_km3_nor_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrEs8otSqdHi"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGUbPgufqdHi"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_k3_cl2 = pd.DataFrame({'fscore':m_nor_k3_cl2.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_nor_cl2)})\n",
        "importances_nor_k3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yoxG6tfsDNN"
      },
      "source": [
        "###### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAxrBYHxsDNN"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIlMcBpQsDNN"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_km3_nor_cl1 = df_nor.loc[df_nor['km_cl_nor_3'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_km3_nor_cl1 = X_km3_nor_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_nor_cl1 = X_km3_nor_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_nor_cl1 = X_km3_nor_cl1.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzL5hUPisDNO"
      },
      "source": [
        "if len(Y_km3_nor_cl1) == len(X_km3_nor_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_km3_nor_cl1))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jnblTj6sDNO"
      },
      "source": [
        "X_km3_nor_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB9RgVwDsDNO"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_nor_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItnDUfFesDNO"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_nor_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snaQEzGesDNP"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(3.29,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLfMQaxHsDNP"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_k3_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_k3_cl1 = cross_validate(xgbrndm_nor_k3_cl1,X_km3_nor_cl1,Y_km3_nor_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emy6y3RcsDNP"
      },
      "source": [
        "cvresults_nor_k3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbU1lPupsDNP"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_k3_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_k3_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_k3_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_k3_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwvXCVTsDNQ"
      },
      "source": [
        "m_nor_k3_cl1 = xgbrndm_nor_k3_cl1.fit(X_km3_nor_cl1,Y_km3_nor_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mmdZmUKsDNQ"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp7KE8oesDNQ"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_k3_cl1 = pd.DataFrame({'fscore':m_nor_k3_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_nor_cl1)})\n",
        "importances_nor_k3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSl2hUXGsq2Y"
      },
      "source": [
        "###### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTevB67Dsq2Y"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeAX1iwKsq2Z"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_km3_nor_cl0 = df_nor.loc[df_nor['km_cl_nor_3'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_km3_nor_cl0 = X_km3_nor_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_km3_nor_cl0 = X_km3_nor_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_km3_nor_cl0 = X_km3_nor_cl0.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaEq1gBDsq2Z"
      },
      "source": [
        "if len(Y_km3_nor_cl0) == len(X_km3_nor_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_km3_nor_cl0))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sT44ICOsq2Z"
      },
      "source": [
        "X_km3_nor_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDw360-Csq2Z"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_km3_nor_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfwbGX7sq2a"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_km3_nor_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiuNqGjzsq2a"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.5,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lisy5kObsq2a"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_k3_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_k3_cl0 = cross_validate(xgbrndm_nor_k3_cl0,X_km3_nor_cl0,Y_km3_nor_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI3R3xEMsq2a"
      },
      "source": [
        "cvresults_nor_k3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mru1FKZbsq2c"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_k3_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_k3_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_k3_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_k3_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vsvUxHEsq2d"
      },
      "source": [
        "m_nor_k3_cl0 = xgbrndm_nor_k3_cl0.fit(X_km3_nor_cl0,Y_km3_nor_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkxPa6Wksq2d"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q9TFYvrsq2d"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_k3_cl0 = pd.DataFrame({'fscore':m_nor_k3_cl0.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_km3_nor_cl0)})\n",
        "importances_nor_k3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b1CuG3KnHcn"
      },
      "source": [
        "## ii. Spectral Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ2xREwUnHc5"
      },
      "source": [
        "### a. All Sample\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3zx6txwnHc5"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import cross_validate\n",
        "from numpy import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGe2M39TnHc5"
      },
      "source": [
        "#### K = 2 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Krp_C7InHc5"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FAiOByMnHc6"
      },
      "source": [
        "X_sup_all_sc2 = df_all.drop([\"km_cl_all_2\",\"km_cl_all_3\",\"km_cl_all_4\", \"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\"sc_cl_all_7\"],axis=1)\n",
        "Y_sup_all_sc2 = df_all[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeCJSaC3nHc6"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_all_sc2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-dIKas-nHc7"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j0128iTnHc7"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_all_sc2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_sc2 = cross_validate(xgbrndm_all_sc2,X_sup_all_sc2,Y_sup_all_sc2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5z9LD9MnHc7"
      },
      "source": [
        "cv_results_sc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_uxOqtNnHc7"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_sc2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_sc2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_sc2['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cv_results_sc2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcVuwL1xnHc8"
      },
      "source": [
        "m = xgbrndm_all_sc2.fit(X_sup_all_sc2,Y_sup_all_sc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmL9N0lonHc8"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm25fLFpnHc8"
      },
      "source": [
        "# Feature importance\n",
        "importance_all_sc2 = pd.DataFrame({'fscore':m.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_all_sc2)})\n",
        "importance_all_sc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deU9lvDKnHc8"
      },
      "source": [
        "#### K = 2 w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XAge-hUnHc9"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBIrzUCCnHc9"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvJz7cd0nHc9"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc2_all_cl1 = df_all.loc[df_all['sc_cl_all_2'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc2_all_cl1 = X_sc2_all_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc2_all_cl1 = X_sc2_all_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc2_all_cl1 = X_sc2_all_cl1.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aXVn_uPnHc9"
      },
      "source": [
        "if len(Y_sc2_all_cl1) == len(X_sc2_all_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(X_sc2_all_cl1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr3gY2eCnHc-"
      },
      "source": [
        "X_sc2_all_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsrEk4H9nHc-"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc2_all_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ifkex6_nHc-"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc2_all_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acpkazl9nHc_"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gffk9i00nHc_"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=random_state)\n",
        "xgbrndm_all_sc2_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_sc2_cl1 = cross_validate(xgbrndm_all_sc2_cl1,X_sc2_all_cl1,Y_sc2_all_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taLpIt8unHc_"
      },
      "source": [
        "cvresults_all_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIJB3UAjnHc_"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_sc2_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_sc2_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_sc2_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_sc2_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoQlMo0RnHdA"
      },
      "source": [
        "m_all_sc2_cl1 = xgbrndm_all_sc2_cl1.fit(X_sc2_all_cl1,Y_sc2_all_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiz7P4qlnHdA"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IymsNWeHnHdA"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_sc2_cl1 = pd.DataFrame({'fscore':m_all_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc2_all_cl1)})\n",
        "importances_all_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xNxY0RMnHdA"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqOvdPvjnHdB"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr-Sj8EHnHdB"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_sc2_all_cl0 = df_all.loc[df_all['sc_cl_all_2'] == 0]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc2_all_cl0 = X_sc2_all_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc2_all_cl0 = X_sc2_all_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc2_all_cl0 = X_sc2_all_cl0.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ueRaXFnHdB"
      },
      "source": [
        "if len(Y_sc2_all_cl0) == len(X_sc2_all_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcO6-y8NFU1J"
      },
      "source": [
        "len(Y_sc2_all_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au3n9ztvnHdB"
      },
      "source": [
        "X_sc2_all_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Uae4lRnHdC"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc2_all_cl0.value_counts(normalize=True)\n",
        "##Only slightly imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0HCA7hUnHdC"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc2_all_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5spRBXQnHdG"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwyRFjT_nHdH"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_sc2_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_sc2_cl0 = cross_validate(xgbrndm_all_sc2_cl0, X_sc2_all_cl0, Y_sc2_all_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v60pxUDCnHdH"
      },
      "source": [
        "cvresults_all_sc2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ9tkxwynHdI"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_sc2_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_sc2_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_sc2_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_sc2_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnIagPWsnHdI"
      },
      "source": [
        "m_all_sc2_cl1 = xgbrndm_all_sc2_cl0.fit(X_sc2_all_cl1,Y_sc2_all_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2d2YHBvnHdI"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7DfG9wqnHdJ"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_sc2_cl1 = pd.DataFrame({'fscore':m_all_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                       'varname': list(X_sc2_all_cl1)})\n",
        "importances_all_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D81GfBNnHdJ"
      },
      "source": [
        "#### K = 3 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Un2eRsnHdJ"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqontwS1nHdK"
      },
      "source": [
        "X_sup_all_sc3 = df_all.drop([\"km_cl_all_2\",\"km_cl_all_3\",\"km_cl_all_4\", \"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_all_2\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\"sc_cl_all_7\"],axis=1)\n",
        "Y_sup_all_sc3 = df_all[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w4w3Tw9nHdK"
      },
      "source": [
        "if len(Y_sup_all_sc3) == len(X_sup_all_sc3):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_sup_all_sc3))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnX6aD59nHdK"
      },
      "source": [
        "X_sup_all_sc3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koDxqXK8nHdK"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sup_all_sc3.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHNSriO5r2le"
      },
      "source": [
        "print(len(Y_sup_all_sc3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG8aA5P2nHdL"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_all_sc3)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5NKSdRjnHdL"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6ff1xsXnHdL"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_sc3 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_all_sc3 = cross_validate(xgbrndm_all_sc3,X_sup_all_sc3,Y_sup_all_sc3,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6iGnhJpnHdM"
      },
      "source": [
        "cv_results_all_sc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esSxb6synHdM"
      },
      "source": [
        "from numpy import mean\n",
        "print(f\"Mean Accuracy = {round(mean(cv_results_all_sc3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_all_sc3['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_all_sc3['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cv_results_all_sc3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aSicXkXnHdM"
      },
      "source": [
        "m_all_sc3 = xgbrndm_all_sc3.fit(X_sup_all_sc3,Y_sup_all_sc3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsL_KQdnHdN"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFV4CsbGnHdN"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_sc3 = pd.DataFrame({'fscore':m_all_sc3.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sup_all_sc3)})\n",
        "importances_all_sc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rXeuJnnHdN"
      },
      "source": [
        "#### K = 3; w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcqirAzWnHdN"
      },
      "source": [
        "##### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in2Z_FI0nHdN"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIgZW3PPnHdO"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc3_all_cl2 = df_all.loc[df_all['sc_cl_all_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc3_all_cl2 = X_sc3_all_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_all_cl2 = X_sc3_all_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_all_cl2 = X_sc3_all_cl2.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMFizalmnHdO"
      },
      "source": [
        "if len(Y_sc3_all_cl2) == len(X_sc3_all_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_sc3_all_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS4Vx6I4nHdO"
      },
      "source": [
        "X_sc3_all_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgfWhW6nHdP"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_all_cl2.value_counts(normalize=True)\n",
        "## Slightly Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYrhB2bUnHdP"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_all_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKcsN5x2nHdP"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk148LE_nHdP"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_sc3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_sc3_cl2 = cross_validate(xgbrndm_all_sc3_cl2,X_sc3_all_cl2,Y_sc3_all_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQPWfLknnHdQ"
      },
      "source": [
        "cvresults_all_sc3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ddRRJGqnHdQ"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_sc3_cl2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_sc3_cl2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_sc3_cl2['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_sc3_cl2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4GcnLm7nHdQ"
      },
      "source": [
        "m_all_sc3_cl2 = xgbrndm_all_sc3_cl2.fit(X_sc3_all_cl2,Y_sc3_all_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmX12Nw4nHdR"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxuVWGiwnHdR"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_sc3_cl2 = pd.DataFrame({'fscore':m_all_sc3_cl2.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_all_cl2)})\n",
        "importances_all_sc3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKhFnTXPnHdS"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noYoH73JnHdS"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbKmsnKjnHdS"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc3_all_cl1 = df_all.loc[df_all['sc_cl_all_3'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc3_all_cl1 = X_sc3_all_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_all_cl1 = X_sc3_all_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_all_cl1 = X_sc3_all_cl1.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6nNEkhMnHdS"
      },
      "source": [
        "if len(Y_sc3_all_cl1) == len(X_sc3_all_cl1):\n",
        "  print(f\"Same Length {len(Y_sc3_all_cl1)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_sc3_all_cl1)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxVD8u0_nHdT"
      },
      "source": [
        "X_sc3_all_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENB2juVGnHdT"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_all_cl1.value_counts(normalize=True)\n",
        "## Roughly Balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUs-Fw_EnHdT"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_all_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVgmOo2VnHdU"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.79,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKRJ5l2UnHdU"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_sc3_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_sc3_cl1 = cross_validate(xgbrndm_all_sc3_cl1,X_sc3_all_cl1,Y_sc3_all_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPtCEMgKnHdU"
      },
      "source": [
        "cvresults_all_sc3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD-GcdERnHdV"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_sc3_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_sc3_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_sc3_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_sc3_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHKuFG72nHdV"
      },
      "source": [
        "m_all_sc3_cl1 = xgbrndm_all_sc3_cl1.fit(X_sc3_all_cl1,Y_sc3_all_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quhcBYnonHdV"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu3VF0mfnHdV"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_sc3_cl1 = pd.DataFrame({'fscore':m_all_sc3_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_all_cl1)})\n",
        "importances_all_sc3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgaX0RMBo4cG"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxv5gMzqo4cG"
      },
      "source": [
        "df_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LGMSExXo4cH"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_sc3_all_cl0 = df_all.loc[df_all['sc_cl_all_3'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_sc3_all_cl0 = X_sc3_all_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_all_cl0 = X_sc3_all_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_all_cl0 = X_sc3_all_cl0.drop([\"km_cl_all_3\",\"km_cl_all_4\",\"km_cl_all_5\",\"km_cl_all_6\",\"km_cl_all_7\",\n",
        "                                    \"sc_cl_all_2\",\"sc_cl_all_3\",\"sc_cl_all_4\",\"sc_cl_all_5\",\"sc_cl_all_6\",\n",
        "                                    \"sc_cl_all_7\",\"km_cl_all_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJuhkLvVo4cH"
      },
      "source": [
        "if len(Y_sc3_all_cl0) == len(X_sc3_all_cl0):\n",
        "  print(f\"Same Length {len(Y_sc3_all_cl0)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_sc3_all_cl0)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-i-5sUYo4cI"
      },
      "source": [
        "X_sc3_all_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZCfPsso4cI"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_all_cl0.value_counts(normalize=True)\n",
        "##Roughl balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xAeEu6ro4cJ"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_all_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5AzpK_No4cJ"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUvgjQ5Oo4cJ"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_all_sc3_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_all_sc3_cl0 = cross_validate(xgbrndm_all_sc3_cl0,X_sc3_all_cl0,Y_sc3_all_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrMiTcsZo4cK"
      },
      "source": [
        "cvresults_all_sc3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFKavl-Vo4cK"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_all_sc3_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_all_sc3_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_all_sc3_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_all_sc3_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgWoGxCAo4cL"
      },
      "source": [
        "m_all_sc3_cl0 = xgbrndm_all_sc3_cl1.fit(X_sc3_all_cl0,Y_sc3_all_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMHRSbSlo4cT"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQDliX9jo4cT"
      },
      "source": [
        "# Feature importance\n",
        "importances_all_sc3_cl0 = pd.DataFrame({'fscore':m_all_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc2_all_cl1)})\n",
        "importances_all_sc3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmuWTJw8oVF4"
      },
      "source": [
        "### b. Escitalopram\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87g-ndgpoVGA"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import cross_validate\n",
        "from numpy import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn0EtgwroVGA"
      },
      "source": [
        "#### K = 2 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhSSK3sFoVGA"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "che5_QJNoVGB"
      },
      "source": [
        "X_sup_esc_sc2 = df_esc.drop([\"km_cl_esc_2\",\"km_cl_esc_3\",\"km_cl_esc_4\", \"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\"sc_cl_esc_7\"],axis=1)\n",
        "Y_sup_esc_sc2 = df_esc[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_1jx8OBoVGB"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_esc_sc2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJrSGlHViPGO"
      },
      "source": [
        "Y_sup_esc_sc2.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWw8vt_Ph-z6"
      },
      "source": [
        "print(len(X_sup_esc_sc2))\n",
        "print(len(Y_sup_esc_sc2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFzlEZN1oVGB"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXZXQnuaoVGB"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_esc_sc2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_sc2 = cross_validate(xgbrndm_esc_sc2,X_sup_esc_sc2,Y_sup_esc_sc2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4jyu7upoVGB"
      },
      "source": [
        "cv_results_sc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPEEbK2loVGB"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_sc2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_sc2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_sc2['test_precision']),3)}\")\n",
        "print(f\"Mean Recall =  {round(mean(cv_results_sc2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UsLRleAoVGB"
      },
      "source": [
        "m = xgbrndm_esc_sc2.fit(X_sup_esc_sc2,Y_sup_esc_sc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uUXGAvtoVGC"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PP4u1GSoVGC"
      },
      "source": [
        "# Feature importance\n",
        "importance_esc_sc2 = pd.DataFrame({'fscore':m.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_esc_sc2)})\n",
        "importance_esc_sc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s6twjemoVGC"
      },
      "source": [
        "#### K = 2 w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3VKtw7soVGC"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvXkecWxoVGC"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr-Requ3oVGC"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc2_esc_cl1 = df_esc.loc[df_esc['sc_cl_esc_2'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc2_esc_cl1 = X_sc2_esc_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc2_esc_cl1 = X_sc2_esc_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc2_esc_cl1 = X_sc2_esc_cl1.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMk6IcoVGC"
      },
      "source": [
        "if len(Y_sc2_esc_cl1) == len(X_sc2_esc_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "len(X_sc2_esc_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvOvmKK7oVGC"
      },
      "source": [
        "X_sc2_esc_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbVHsG03oVGD"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc2_esc_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9gbImfIoVGD"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc2_esc_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E1sBmSFoVGE"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAFhOgg-oVGE"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=random_state)\n",
        "xgbrndm_esc_sc2_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_sc2_cl1 = cross_validate(xgbrndm_esc_sc2_cl1,X_sc2_esc_cl1,Y_sc2_esc_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pI_-5W6oVGE"
      },
      "source": [
        "cvresults_esc_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IgyxdutoVGE"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_sc2_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_sc2_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_sc2_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recesc = {round(mean(cvresults_esc_sc2_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw1S5TCJoVGE"
      },
      "source": [
        "m_esc_sc2_cl1 = xgbrndm_esc_sc2_cl1.fit(X_sc2_esc_cl1,Y_sc2_esc_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ifzDPXWoVGE"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6YWLlqAoVGF"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_sc2_cl1 = pd.DataFrame({'fscore':m_esc_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc2_esc_cl1)})\n",
        "importances_esc_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud12OyE8oVGF"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYumxKfWoVGF"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P64WxZHcoVGF"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_sc2_esc_cl0 = df_esc.loc[df_esc['sc_cl_esc_2'] == 0]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc2_esc_cl0 = X_sc2_esc_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc2_esc_cl0 = X_sc2_esc_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc2_esc_cl0 = X_sc2_esc_cl0.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrHPF8AvoVGF"
      },
      "source": [
        "if len(Y_sc2_esc_cl0) == len(X_sc2_esc_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxGaF27EoVGF"
      },
      "source": [
        "X_sc2_esc_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7ESoQTmoVGG"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc2_esc_cl0.value_counts(normalize=True)\n",
        "##Only slightly imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no3FCIb9oVGG"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc2_esc_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhdF5LcvoVGG"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iloQVRkhoVGG"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_sc2_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_sc2_cl0 = cross_validate(xgbrndm_esc_sc2_cl0, X_sc2_esc_cl0, Y_sc2_esc_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgyhQ_69oVGG"
      },
      "source": [
        "cvresults_esc_sc2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_eMwHHSoVGH"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_sc2_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_sc2_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_sc2_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recesc = {round(mean(cvresults_esc_sc2_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI8KL3MMoVGH"
      },
      "source": [
        "m_esc_sc2_cl1 = xgbrndm_esc_sc2_cl0.fit(X_sc2_esc_cl1,Y_sc2_esc_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOHwPgCFoVGH"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00K7X3_ToVGH"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_sc2_cl1 = pd.DataFrame({'fscore':m_esc_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                       'varname': list(X_sc2_esc_cl1)})\n",
        "importances_esc_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVOOjq-poVGH"
      },
      "source": [
        "#### K = 3 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-DYpc5zoVGH"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDLM4Z1moVGI"
      },
      "source": [
        "X_sup_esc_sc3 = df_esc.drop([\"km_cl_esc_2\",\"km_cl_esc_3\",\"km_cl_esc_4\", \"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_esc_2\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\"sc_cl_esc_7\"],axis=1)\n",
        "Y_sup_esc_sc3 = df_esc[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRcXZ6u8oVGI"
      },
      "source": [
        "if len(Y_sup_esc_sc3) == len(X_sup_esc_sc3):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNq54NntoVGI"
      },
      "source": [
        "X_sup_esc_sc3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iO1OIBioVGI"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sup_esc_sc3.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8AVsg89oVGJ"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_esc_sc3)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NOaFIMjoVGJ"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSHpyyJloVGJ"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_sc3 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_esc_sc3 = cross_validate(xgbrndm_esc_sc3,X_sup_esc_sc3,Y_sup_esc_sc3,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBvwPDixoVGJ"
      },
      "source": [
        "cv_results_esc_sc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWhIdUjsoVGJ"
      },
      "source": [
        "from numpy import mean\n",
        "print(f\"Mean Accuracy = {round(mean(cv_results_esc_sc3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_esc_sc3['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_esc_sc3['test_precision']),3)}\")\n",
        "print(f\"Mean Recesc = {round(mean(cv_results_esc_sc3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deoioRXUoVGK"
      },
      "source": [
        "m_esc_sc3 = xgbrndm_esc_sc3.fit(X_sup_esc_sc3,Y_sup_esc_sc3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZnq6C05oVGK"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dguz__1oVGK"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_sc3 = pd.DataFrame({'fscore':m_esc_sc3.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sup_esc_sc3)})\n",
        "importances_esc_sc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEgEjfwoVGK"
      },
      "source": [
        "#### K = 3; w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTyccfPfoVGN"
      },
      "source": [
        "##### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V18etjc0oVGN"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XANEtcWjoVGN"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc3_esc_cl2 = df_esc.loc[df_esc['sc_cl_esc_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc3_esc_cl2 = X_sc3_esc_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_esc_cl2 = X_sc3_esc_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_esc_cl2 = X_sc3_esc_cl2.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhP7RSDeoVGN"
      },
      "source": [
        "if len(Y_sc3_esc_cl2) == len(X_sc3_esc_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_sc3_esc_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI73p8aHoVGN"
      },
      "source": [
        "X_sc3_esc_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksQpU82aoVGO"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_esc_cl2.value_counts(normalize=True)\n",
        "## Slightly Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yUhqjbIoVGO"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_esc_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0raSJARFoVGO"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.2,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH5tbfbFoVGO"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_sc3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_sc3_cl2 = cross_validate(xgbrndm_esc_sc3_cl2,X_sc3_esc_cl2,Y_sc3_esc_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6n-OG8roVGO"
      },
      "source": [
        "cvresults_esc_sc3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvFyiyB8oVGO"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_sc3_cl2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_sc3_cl2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_sc3_cl2['test_precision']),3)}\")\n",
        "print(f\"Mean Recesc = {round(mean(cvresults_esc_sc3_cl2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04xT9oOWoVGO"
      },
      "source": [
        "m_esc_sc3_cl2 = xgbrndm_esc_sc3_cl2.fit(X_sc3_esc_cl2,Y_sc3_esc_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V99AH13poVGP"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWCFSI3LoVGP"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_sc3_cl2 = pd.DataFrame({'fscore':m_esc_sc3_cl2.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_esc_cl2)})\n",
        "importances_esc_sc3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_ll_kk5oVGP"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4MxaCRFoVGP"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sme21jxroVGP"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc3_esc_cl1 = df_esc.loc[df_esc['sc_cl_esc_3'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc3_esc_cl1 = X_sc3_esc_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_esc_cl1 = X_sc3_esc_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_esc_cl1 = X_sc3_esc_cl1.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbprLONkoVGP"
      },
      "source": [
        "if len(Y_sc3_esc_cl1) == len(X_sc3_esc_cl1):\n",
        "  print(f\"Same Length {len(Y_sc3_esc_cl1)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_sc3_esc_cl1)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-csG9e_qoVGQ"
      },
      "source": [
        "X_sc3_esc_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWc8jvJUoVGQ"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_esc_cl1.value_counts(normalize=True)\n",
        "## Roughly Balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9g_nCfloVGQ"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_esc_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cHLeI36oVGQ"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.79,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO9VteHcoVGQ"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_sc3_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_sc3_cl1 = cross_validate(xgbrndm_esc_sc3_cl1,X_sc3_esc_cl1,Y_sc3_esc_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNwv46wXoVGQ"
      },
      "source": [
        "cvresults_esc_sc3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqbVP4WUoVGQ"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_sc3_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_sc3_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_sc3_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recesc = {round(mean(cvresults_esc_sc3_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqLxEJfoVGR"
      },
      "source": [
        "m_esc_sc3_cl1 = xgbrndm_esc_sc3_cl1.fit(X_sc3_esc_cl1,Y_sc3_esc_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdRIcb9roVGR"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwduXYMoVGR"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_sc3_cl1 = pd.DataFrame({'fscore':m_esc_sc3_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_esc_cl1)})\n",
        "importances_esc_sc3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aRuZB1aqrA7"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCUwMnsJqrBI"
      },
      "source": [
        "df_esc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uPsVc3nqrBJ"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_sc3_esc_cl0 = df_esc.loc[df_esc['sc_cl_esc_3'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_sc3_esc_cl0 = X_sc3_esc_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_esc_cl0 = X_sc3_esc_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_esc_cl0 = X_sc3_esc_cl0.drop([\"km_cl_esc_3\",\"km_cl_esc_4\",\"km_cl_esc_5\",\"km_cl_esc_6\",\"km_cl_esc_7\",\n",
        "                                    \"sc_cl_esc_2\",\"sc_cl_esc_3\",\"sc_cl_esc_4\",\"sc_cl_esc_5\",\"sc_cl_esc_6\",\n",
        "                                    \"sc_cl_esc_7\",\"km_cl_esc_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDaCh4y5qrBJ"
      },
      "source": [
        "if len(Y_sc3_esc_cl0) == len(X_sc3_esc_cl0):\n",
        "  print(f\"Same Length {len(Y_sc3_esc_cl0)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_sc3_esc_cl0)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULqUnBFqrBJ"
      },
      "source": [
        "X_sc3_esc_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHsrczONqrBK"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_esc_cl0.value_counts(normalize=True)\n",
        "##Very Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "warMPEILqrBK"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_esc_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niLdgKFSqrBK"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTYqhAYkqrBK"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_esc_sc3_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_esc_sc3_cl0 = cross_validate(xgbrndm_esc_sc3_cl0,X_sc3_esc_cl0,Y_sc3_esc_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfB_wJ4-qrBN"
      },
      "source": [
        "cvresults_esc_sc3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F25UOXbtqrBO"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_esc_sc3_cl0['test_accuracy']),2)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_esc_sc3_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_esc_sc3_cl0['test_precision']),2)}\")\n",
        "print(f\"Mean Recesc = {round(mean(cvresults_esc_sc3_cl0['test_recall']),2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "basp9O3gqrBO"
      },
      "source": [
        "m_esc_sc3_cl0 = xgbrndm_esc_sc3_cl1.fit(X_sc3_esc_cl0,Y_sc3_esc_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92GQ9RfkqrBO"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfLcGE6CqrBP"
      },
      "source": [
        "# Feature importance\n",
        "importances_esc_sc3_cl0 = pd.DataFrame({'fscore':m_esc_sc3_cl0.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_esc_cl0)})\n",
        "importances_esc_sc3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdRSDizuI-Ft"
      },
      "source": [
        "### c. Nortryptaline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R50zBPuGI-F6"
      },
      "source": [
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import cross_validate\n",
        "from numpy import mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNwgdRnqI-F6"
      },
      "source": [
        "#### K = 2 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nerNuWm1I-F6"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbfvnE2WI-F7"
      },
      "source": [
        "X_sup_nor_sc2 = df_nor.drop([\"km_cl_nor_2\",\"km_cl_nor_3\",\"km_cl_nor_4\", \"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\"sc_cl_nor_7\"],axis=1)\n",
        "Y_sup_nor_sc2 = df_nor[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7psLapSUI-F7"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_nor_sc2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5BrXmCJI-F7"
      },
      "source": [
        "Y_sup_nor_sc2.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY9gScj_I-F8"
      },
      "source": [
        "print(len(X_sup_nor_sc2))\n",
        "print(len(Y_sup_nor_sc2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHL5IgvMI-F8"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9De0nRbI-F8"
      },
      "source": [
        "xgb = XGBClassifier(random_state=123)\n",
        "xgbrndm_nor_sc2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = random_state, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_nor_sc2 = cross_validate(xgbrndm_nor_sc2,X_sup_nor_sc2,Y_sup_nor_sc2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaBJLTHyI-F8"
      },
      "source": [
        "cv_results_nor_sc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAcg2PeuI-F9"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cv_results_nor_sc2['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_nor_sc2['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_nor_sc2['test_precision']),3)}\")\n",
        "print(f\"Mean Recall =  {round(mean(cv_results_nor_sc2['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQoB-uAEI-F9"
      },
      "source": [
        "m_nor_sc2 = xgbrndm_nor_sc2.fit(X_sup_nor_sc2,Y_sup_nor_sc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gznyTsM7I-F9"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kci8x1B-I-F-"
      },
      "source": [
        "# Feature importance\n",
        "importance_nor_sc2 = pd.DataFrame({'fscore':m_nor_sc2.best_estimator_.feature_importances_,\n",
        "                           'varname': list(X_sup_nor_sc2)})\n",
        "importance_nor_sc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUnlm1GEI-GB"
      },
      "source": [
        "#### K = 2 w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMeAhCBlI-GB"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2u9layaI-GB"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYBxfHR9I-GB"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc2_nor_cl1 = df_nor.loc[df_nor['sc_cl_nor_2'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc2_nor_cl1 = X_sc2_nor_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc2_nor_cl1 = X_sc2_nor_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc2_nor_cl1 = X_sc2_nor_cl1.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0gbRFaXI-GC"
      },
      "source": [
        "if len(Y_sc2_nor_cl1) == len(X_sc2_nor_cl1):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "len(X_sc2_nor_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCAJQWNgI-GC"
      },
      "source": [
        "X_sc2_nor_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyvecEGbI-GC"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc2_nor_cl1.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EeIsVgmI-GD"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc2_nor_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YCfAyoAI-GD"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcQVOylhI-GD"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=random_state)\n",
        "xgbrndm_nor_sc2_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_sc2_cl1 = cross_validate(xgbrndm_nor_sc2_cl1,X_sc2_nor_cl1,Y_sc2_nor_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQwXPDaaI-GD"
      },
      "source": [
        "cvresults_nor_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PXyLcOHI-GE"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_sc2_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_sc2_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_sc2_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_sc2_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDUDqJvI-GE"
      },
      "source": [
        "m_nor_sc2_cl1 = xgbrndm_nor_sc2_cl1.fit(X_sc2_nor_cl1,Y_sc2_nor_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP-g7Z3lI-GE"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua28fbsAI-GE"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_sc2_cl1 = pd.DataFrame({'fscore':m_nor_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc2_nor_cl1)})\n",
        "importances_nor_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWjuGJQbI-GF"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHdnQZ2lI-GF"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpJOr_NI-GF"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 2:\n",
        "X_sc2_nor_cl0 = df_nor.loc[df_nor['sc_cl_nor_2'] == 0]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc2_nor_cl0 = X_sc2_nor_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc2_nor_cl0 = X_sc2_nor_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc2_nor_cl0 = X_sc2_nor_cl0.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67W6Dq_KI-GG"
      },
      "source": [
        "if len(Y_sc2_nor_cl0) == len(X_sc2_nor_cl0):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJBCTtzI-GG"
      },
      "source": [
        "X_sc2_nor_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4WpcU7TI-GG"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc2_nor_cl0.value_counts(normalize=True)\n",
        "##Only slightly imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRznWe-NI-GG"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc2_nor_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiL1p5kLI-GG"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXJ9OdSRI-GH"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_sc2_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_sc2_cl0 = cross_validate(xgbrndm_nor_sc2_cl0, X_sc2_nor_cl0, Y_sc2_nor_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6G80GaI-GH"
      },
      "source": [
        "cvresults_nor_sc2_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC-4BZJ4I-GH"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_sc2_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_sc2_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_sc2_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_sc2_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcVat-ESI-GH"
      },
      "source": [
        "m_nor_sc2_cl1 = xgbrndm_nor_sc2_cl0.fit(X_sc2_nor_cl1,Y_sc2_nor_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFwBQ_sII-GI"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MTZR7OQI-GJ"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_sc2_cl1 = pd.DataFrame({'fscore':m_nor_sc2_cl1.best_estimator_.feature_importances_,\n",
        "                                       'varname': list(X_sc2_nor_cl1)})\n",
        "importances_nor_sc2_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ78zwaFI-GJ"
      },
      "source": [
        "#### K = 3 ; w/cluster var"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdnmfyvLI-GJ"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZwLNZUXI-GK"
      },
      "source": [
        "X_sup_nor_sc3 = df_nor.drop([\"km_cl_nor_2\",\"km_cl_nor_3\",\"km_cl_nor_4\", \"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\"mdpercadj\",\"hdremit.all\",\"sc_cl_nor_2\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\"sc_cl_nor_7\"],axis=1)\n",
        "Y_sup_nor_sc3 = df_nor[\"hdremit.all\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cqB0_-MI-GK"
      },
      "source": [
        "if len(Y_sup_nor_sc3) == len(X_sup_nor_sc3):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q426FDpfI-GK"
      },
      "source": [
        "X_sup_nor_sc3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8vKDPUtI-GK"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sup_nor_sc3.value_counts(normalize=True)\n",
        "##Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHaWlHhdI-GK"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sup_nor_sc3)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rfW0UlVI-GL"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa71V-bwI-GL"
      },
      "source": [
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_sc3 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cv_results_nor_sc3 = cross_validate(xgbrndm_nor_sc3,X_sup_nor_sc3,Y_sup_nor_sc3,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucPcEZb2I-GL"
      },
      "source": [
        "cv_results_nor_sc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VSX3lt6I-GL"
      },
      "source": [
        "from numpy import mean\n",
        "print(f\"Mean Accuracy = {round(mean(cv_results_nor_sc3['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cv_results_nor_sc3['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cv_results_nor_sc3['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cv_results_nor_sc3['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGADxudsI-GM"
      },
      "source": [
        "m_nor_sc3 = xgbrndm_nor_sc3.fit(X_sup_nor_sc3,Y_sup_nor_sc3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hZ3rymvI-GM"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UyZSH9CI-GM"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_sc3 = pd.DataFrame({'fscore':m_nor_sc3.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sup_nor_sc3)})\n",
        "importances_nor_sc3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVaPeiw3I-GM"
      },
      "source": [
        "#### K = 3; w/o cluster var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rubKXgu5I-GM"
      },
      "source": [
        "##### Cluster 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG845asYI-GN"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daTkGeKkI-GN"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc3_nor_cl2 = df_nor.loc[df_nor['sc_cl_nor_3'] == 2]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc3_nor_cl2 = X_sc3_nor_cl2.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_nor_cl2 = X_sc3_nor_cl2.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_nor_cl2 = X_sc3_nor_cl2.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGtaD-LhI-GO"
      },
      "source": [
        "if len(Y_sc3_nor_cl2) == len(X_sc3_nor_cl2):\n",
        "  print(\"Same Length\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(len(Y_sc3_nor_cl2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kZClMsxI-GO"
      },
      "source": [
        "X_sc3_nor_cl2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvCfF8pWI-GO"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_nor_cl2.value_counts(normalize=True)\n",
        "## Slightly Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWQ6bdRHI-GP"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_nor_cl2)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jA9qFExI-GP"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.2,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMrJMM4qI-GP"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_sc3_cl2 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_sc3_cl2 = cross_validate(xgbrndm_nor_sc3_cl2,X_sc3_nor_cl2,Y_sc3_nor_cl2,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gunBSRpI-GP"
      },
      "source": [
        "cvresults_nor_sc3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL-09TNsI-GP"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_sc3_cl2['test_accuracy']),2)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_sc3_cl2['test_roc_auc']),2)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_sc3_cl2['test_precision']),2)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_sc3_cl2['test_recall']),2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t_qFYwbI-GQ"
      },
      "source": [
        "m_nor_sc3_cl2 = xgbrndm_nor_sc3_cl2.fit(X_sc3_nor_cl2,Y_sc3_nor_cl2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Gi_vm3I-GQ"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA91L6ATI-GQ"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_sc3_cl2 = pd.DataFrame({'fscore':m_nor_sc3_cl2.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_nor_cl2)})\n",
        "importances_nor_sc3_cl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwau2nyHI-GQ"
      },
      "source": [
        "##### Cluster 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03ZCTTD_I-GQ"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bFBF53xI-GR"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 1:\n",
        "X_sc3_nor_cl1 = df_nor.loc[df_nor['sc_cl_nor_3'] == 1]\n",
        "# Subsetting Outcome for Cluster 1:\n",
        "Y_sc3_nor_cl1 = X_sc3_nor_cl1.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_nor_cl1 = X_sc3_nor_cl1.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_nor_cl1 = X_sc3_nor_cl1.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BRkDdgXI-GR"
      },
      "source": [
        "if len(Y_sc3_nor_cl1) == len(X_sc3_nor_cl1):\n",
        "  print(f\"Same Length {len(Y_sc3_nor_cl1)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_sc3_nor_cl1)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY79N4bAI-GR"
      },
      "source": [
        "X_sc3_nor_cl1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE7u6P1_I-GR"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_nor_cl1.value_counts(normalize=True)\n",
        "## Roughly Balanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MqEn9jnI-GV"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_nor_cl1)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx6U02Y-I-GV"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(1.79,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtWf_vxbI-GW"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_sc3_cl1 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_sc3_cl1 = cross_validate(xgbrndm_nor_sc3_cl1,X_sc3_nor_cl1,Y_sc3_nor_cl1,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abQ8Z5QHI-GW"
      },
      "source": [
        "cvresults_nor_sc3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maRrluaEI-GW"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_sc3_cl1['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_sc3_cl1['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_sc3_cl1['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_sc3_cl1['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODQ6CjyhI-GX"
      },
      "source": [
        "m_nor_sc3_cl1 = xgbrndm_nor_sc3_cl1.fit(X_sc3_nor_cl1,Y_sc3_nor_cl1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KRAEPSFI-GX"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eui2c7t3I-GX"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_sc3_cl1 = pd.DataFrame({'fscore':m_nor_sc3_cl1.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_nor_cl1)})\n",
        "importances_nor_sc3_cl1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwGKlOC5I-GX"
      },
      "source": [
        "##### Cluster 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7TvT8TrI-GX"
      },
      "source": [
        "df_nor.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTPfmufxI-GY"
      },
      "source": [
        "# Splitting data into X and Y\n",
        "# Subsetting Cluster 0:\n",
        "X_sc3_nor_cl0 = df_nor.loc[df_nor['sc_cl_nor_3'] == 0]\n",
        "# Subsetting Outcome for Cluster 0:\n",
        "Y_sc3_nor_cl0 = X_sc3_nor_cl0.loc[:,\"hdremit.all\"]\n",
        "# Dropping Outcome Variables from X\n",
        "X_sc3_nor_cl0 = X_sc3_nor_cl0.drop([\"hdremit.all\",\"mdpercadj\"],axis=1)\n",
        "# Dropping Other Cluster Vars from X\n",
        "X_sc3_nor_cl0 = X_sc3_nor_cl0.drop([\"km_cl_nor_3\",\"km_cl_nor_4\",\"km_cl_nor_5\",\"km_cl_nor_6\",\"km_cl_nor_7\",\n",
        "                                    \"sc_cl_nor_2\",\"sc_cl_nor_3\",\"sc_cl_nor_4\",\"sc_cl_nor_5\",\"sc_cl_nor_6\",\n",
        "                                    \"sc_cl_nor_7\",\"km_cl_nor_2\"],axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMP9dSiUI-GY"
      },
      "source": [
        "if len(Y_sc3_nor_cl0) == len(X_sc3_nor_cl0):\n",
        "  print(f\"Same Length {len(Y_sc3_nor_cl0)}\")\n",
        "else:\n",
        "  print(\"Not Mathcing\")\n",
        "\n",
        "print(f\"The cluster contains {len(X_sc3_nor_cl0)} patients \")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4hwRZK2I-GY"
      },
      "source": [
        "X_sc3_nor_cl0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKgZ8NRtI-GY"
      },
      "source": [
        "# Checking for Imbalance: \n",
        "Y_sc3_nor_cl0.value_counts(normalize=True)\n",
        "##Very Imbalanced"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT2qabdII-GZ"
      },
      "source": [
        "from collections import Counter\n",
        "# estimating the scale_pos_weight value to include in grid search.\n",
        "counter = Counter(Y_sc3_nor_cl0)\n",
        "estimate = counter[0] / counter[1]\n",
        "print('Estimate: %.3f' % estimate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4DAuA6hI-GZ"
      },
      "source": [
        "import numpy as np\n",
        "# Defining hyperparameter search grids\n",
        "\n",
        "## Positive Scaling (for class imbalance)\n",
        "xgb_scale_pos_weight = [int(x) for x in np.linspace(estimate,20,10)]\n",
        "\n",
        "## Number of trees to be used\n",
        "xgb_n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
        "\n",
        "## Maximum number of levels in tree\n",
        "xgb_max_depth = [int(x) for x in np.linspace(2, 20, 10)]\n",
        "\n",
        "## Minimum number of instances needed in each node\n",
        "xgb_min_child_weight = [int(x) for x in np.linspace(1, 10, 10)]\n",
        "\n",
        "## Learning rate\n",
        "xgb_eta = [x for x in np.linspace(0.1, 0.6, 6)]\n",
        "\n",
        "# Learning objective used\n",
        "xgb_objective = ['reg:logistic']\n",
        "\n",
        "# Evaluation metric used \n",
        "xgb_eval_metric = ['aucpr'] \n",
        "\n",
        "# Create the grid\n",
        "xgb_grid = {'n_estimators': xgb_n_estimators,\n",
        "            'max_depth': xgb_max_depth,\n",
        "            'min_child_weight': xgb_min_child_weight,\n",
        "            'scale_pos_weight': xgb_scale_pos_weight, \n",
        "            'eta': xgb_eta,\n",
        "            'objective': xgb_objective,\n",
        "            'eval_metric': xgb_eval_metric}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyCWa-j7I-GZ"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(randome_state=123)\n",
        "xgbrndm_nor_sc3_cl0 = RandomizedSearchCV(estimator = xgb, param_distributions = xgb_grid, \n",
        "                                       n_iter = 100, cv = 5, verbose = 2, \n",
        "                                       random_state = 123, \n",
        "                                       return_train_score = True,\n",
        "                                       n_jobs = -1)\n",
        "\n",
        "cvresults_nor_sc3_cl0 = cross_validate(xgbrndm_nor_sc3_cl0,X_sc3_nor_cl0,Y_sc3_nor_cl0,cv=5,\n",
        "                            scoring = ('accuracy','precision','recall','roc_auc'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaveRGwVI-GZ"
      },
      "source": [
        "cvresults_nor_sc3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaECcQneI-GZ"
      },
      "source": [
        "print(f\"Mean Accuracy = {round(mean(cvresults_nor_sc3_cl0['test_accuracy']),3)}\")\n",
        "print(f\"Mean AUC = {round(mean(cvresults_nor_sc3_cl0['test_roc_auc']),3)}\")\n",
        "print(f\"Mean Precision = {round(mean(cvresults_nor_sc3_cl0['test_precision']),3)}\")\n",
        "print(f\"Mean Recall = {round(mean(cvresults_nor_sc3_cl0['test_recall']),3)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzgGMDXzI-Gd"
      },
      "source": [
        "m_nor_sc3_cl0 = xgbrndm_nor_sc3_cl0.fit(X_sc3_nor_cl0,Y_sc3_nor_cl0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIzbYlfxI-Gd"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhTz7IKmI-Gd"
      },
      "source": [
        "# Feature importance\n",
        "importances_nor_sc3_cl0 = pd.DataFrame({'fscore':m_nor_sc3_cl0.best_estimator_.feature_importances_,\n",
        "                                     'varname': list(X_sc3_nor_cl0)})\n",
        "importances_nor_sc3_cl0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}